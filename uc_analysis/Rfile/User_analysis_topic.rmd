---
title: "User_analysis"
author: "qingye"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readxl)
library(knitr)
library(xtable)
library(ggplot2)
library(reshape2)
library(scales)
library(plyr)
library(treemap)
library(grid)

source('~/Rfile/R_impala.R')
dateend = Sys.Date()-1
datestart = Sys.Date()-2
datestart.str = format(datestart,'%Y%m%d')
dateend.str = format(dateend,'%Y%m%d')
dates = datestart.str:dateend.str
ldateend = Sys.Date()-3
ldatestart = Sys.Date()-7
ldatestart.str = format(ldatestart,'%Y%m%d')
ldateend.str = format(ldateend,'%Y%m%d')
activity_name = "双十一"
compare_period = "近一周"
options(scipen = 10)

```
#本报告对`r activity_name`活动，`r datestart`至`r dateend` 两天用户的行为进行了分析
####包括反映用户数目的UV,反映用户活跃度的：人均PV,用户活跃时长,以及反映用户黏度的次日留存，本次报告加入了路径深度的分析,加入了转化部分,并采用多种方式作图
####本次报告用R语言自动化生成报告的框架搭建，今后将在此基础上不断改进优化，下次报告将深化转化方面的分析(加入渠道分析)，加深对于路径方面的分析，引进智能化的异常值提醒和分析，考虑加入排名变动


##近两周的日活跃度(UV)
```{r daily_pvuv,echo = FALSE}
#1daily activity pvuv
pvuv_sql = paste0("select dt,
                 count(1) uv,
                 sum(pv) pv ,
                 sum(pv)/count(1) perpv,
                 count(if(isnew='new',1,null)) newuv ,
                 sum(if(isnew='new',pv,0)) newpv, 
                 sum(if(isnew='new',pv,0))/count(if(isnew='new',1,null)) newperpv,
                 count(if(isnew='old',1,null)) olduv ,
                 sum(if(isnew='old',pv,0)) oldpv, 
                 sum(if(isnew='old',pv,0))/count(if(isnew='old',1,null)) oldperpv
                 from 
                 dl.umid_pv where dt>='",ldatestart.str,
                 "'group by dt")
mpvuv = read_data_impala_general(pvuv_sql)
mpvuv = mpvuv[order(mpvuv$dt),]
xlarge = nrow(mpvuv)
xtopic = xlarge - 2
pvuv = mpvuv[xtopic:xlarge,]
wpvuv = pvuv
pvuv$fdt = as.factor(pvuv$dt)
matrix = as.matrix(cbind(newuv = pvuv$newuv,olduv = pvuv$olduv))
matrix = t(matrix)
maxy = max(pvuv$uv)
maxx = pvuv[which.max(pvuv$uv),"dt"]
meany = mean(pvuv$uv)
newbetterthanold = sum(pvuv$newuv>pvuv$olduv)
abnormalhighdt = pvuv[pvuv$uv>1.3*meany,"dt"]
abnormallowdt = pvuv[pvuv$uv<0.7*meany,"dt"]
par(bg = 'white')
colors <- c("green","red")
dates <- pvuv$dt
stacks <- c("New","Old")
# Create the bar chart.
barplot(matrix,main="Every day uv",names.arg=dates,xlab="date",ylab="uv",col=colors)
# Add the legend to the chart.
legend(x = 0,y = maxy, stacks, cex=0.7, fill=colors,xjust = 0, yjust = 1)
```

平均UV为`r round(meany)`,

最大UV为`r round(maxy)`,

最大值发生在`r maxx`

有`r newbetterthanold` 天新用户在UV值上超过老用户

`r ifelse(length(abnormalhighdt)==0,"并没有哪天",abnormalhighdt)` 用户UV明显高于平均值
`r ifelse(length(abnormallowdt)==0,"并没有哪天",abnormallowdt)` 用户UV明显低于平均值


##`r activity_name`人均PV
```{r pv_perperson,echo = FALSE}
# par(bg = 'black')
colors = rainbow(3)
ylarge = max(max(pvuv$perpv),max(pvuv$newperpv),max(pvuv$oldperpv))
tmax = max(pvuv$perpv)
dttmax = pvuv[which.max(pvuv$perpv),"dt"]
tmin = min(pvuv$perpv)
dttmin = pvuv[which.min(pvuv$perpv),"dt"]
tmean = mean(pvuv$perpv)

nmax = max(pvuv$newperpv)
dtnmax = pvuv[which.max(pvuv$newperpv),"dt"]
nmin = min(pvuv$newperpv)
dtnmin = pvuv[which.min(pvuv$newperpv),"dt"]
nmean = mean(pvuv$newperpv)

omax = max(pvuv$oldperpv)
dtomax = pvuv[which.max(pvuv$oldperpv),"dt"]
omin = min(pvuv$oldperpv)
dtomin = pvuv[which.min(pvuv$oldperpv),"dt"]
omean = mean(pvuv$oldperpv)

newbetterthanold = sum(pvuv$newperpv>pvuv$oldperpv)
difftrend = pvuv[!(c(1,sign(diff(pvuv$newperpv)*diff(pvuv$oldperpv)))+1),"dt"]
matrix = as.matrix(cbind(newuv = pvuv$newperpv,olduv = pvuv$oldperpv))
matrix = t(matrix)
#考虑人均pv
par(bg = 'white')
colors <- c("green","red")
dates <- pvuv$dt
stacks <- c("New","Old")
# Create the bar chart.
barplot(matrix,main="每日人均PV",names.arg=dates,xlab="date",ylab="人均pv",col=colors,beside = TRUE)
legend('topright',c("newperpv","oldperpv"),cex = 0.7,fill = colors,text.col= 'black')
```

总体人均PV均值为`r tmean` 

总体人均PV最大值为`r tmax` 最大值发生在 `r dttmax`

总体人均PV最小值为`r tmin` 最小值发生在 `r dttmin`

新用户人均PV均值为`r nmean` 

新用户人均PV最大值为`r nmax` 最大值发生在 `r dtnmax`

新用户人均PV最小值为`r nmin` 最小值发生在 `r dtnmin`

老用户人均PV均值为`r omean` 

老用户人均PV最大值为`r omax` 最大值发生在 `r dtomax`

老用户人均PV最小值为`r omin` 最小值发生在 `r dtomin`

有`r newbetterthanold` 天新用户在人均PV值上超过老用户

`r difftrend` 新用户和老用户的趋势不一样

##PV 趋势图，篮框内为聚焦时期
```{r pvuv_trend, echo=FALSE}
pm <- ggplot(wpvuv, aes(as.Date(dt,format = "%Y%m%d"), pv)) + labs(x = "DATE",y = "PV") + ylim(0,max(wpvuv$pv,na.rm = T))
mainplot <- pm + geom_line(colour = I("purple")) + labs(title = paste0(activity_name,"PV趋势变化"))
p = ggplot(mpvuv,aes(as.Date(dt,format = "%Y%m%d"), pv))+labs(x = "DATE",y = "PV") + ylim(min(mpvuv$pv,na.rm = T),max(mpvuv$pv,na.rm = T))
p1 <- p + geom_rect(aes(xmin = as.Date(mpvuv$dt[xtopic],format = "%Y%m%d"), xmax = as.Date(mpvuv$dt[xlarge],format = "%Y%m%d"),
                        ymin = min(mpvuv$pv, na.rm = TRUE), ymax = max(mpvuv$pv, na.rm = TRUE)),fill = alpha("lightblue", 0.2))
subplot <- p1 + geom_line(colour = I("grey"),size = 0.8) 
vp <- viewport(width = 0.4, height = 0.4, x = 1,
               y = unit(0.7, "lines"), just = c("right","bottom"))
full <- function() {
  print(mainplot)
  print(subplot, vp = vp)
}
full()
```

画中画分析，大图，小图高亮的那部分，表示示`r activity_name`pv趋势，而小图表示`r compare_period`的pv情况
`r activity_name`比过去`r compare_period`整体pv要高

## 用户活跃时长
```{r time_span, echo=FALSE}
time_span_sql = paste0("select dt,
  avg(persvg) totalavg,
  avg(case when isnew ='new' then persvg else null end) newavg,
  avg(case when isnew ='old' then persvg else null end) oldavg
from
(
  select a.dt,a.u_mid,
  sum(CAST(p_stay_time AS INT))/1000/60 persvg,
  case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end isnew 
  from 
  ods.ods_app_pageview_info a 
  left outer join 
  dl.umid_firstonlinetime b on a.u_mid=b.u_mid 
  where a.dt>=",ldateend.str," and b.dt='",dateend.str,"' and
  p_domain='mmall.com' and service like '%staytime%' and substr(a.u_mid,1,2)!='a_' and path='z'  and l_city!='测试'
  and p_type not in ('page.closedown','page.wakeup','page.activate.main') and length(p_stay_time)<=7
  group by a.dt,a.u_mid,case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end 
)a group by dt")
  time_span = read_data_impala_general(time_span_sql)
  time_span = time_span[order(time_span$dt),]
  # write.xlsx(time_span,"~/data/uc_analysis/time_span.xlsx")
  time_span$fdt = as.factor(time_span$dt)
  time_span.new = time_span[,-1]
  timespan.m <- melt(time_span.new)
  timespan.m <- ddply(timespan.m, .(variable), transform,rescale = rescale(value))
  timespan.m$dates = str_sub(as.character(timespan.m$fdt),5,8)
  names(timespan.m) = c("fdt","old_new_user","time_span","rescale","dates")
  (p <- ggplot(timespan.m, aes(dates,old_new_user)) + geom_tile(aes(fill = time_span),colour = "white") + scale_fill_gradient(low = "white",high = "purple"))
```

对于用户访问时长作的热力图，可以看出新老用户差距明显，新用户大部分日子访问时长接近白色
双十一当天的用户访问时长明显高过11月10日以及11月12日，无论新用户还是老用户

## 次日留存
```{r survival,echo=FALSE}
  survival_sql = paste0("select 
  a.dt,ndv(a.u_mid) t,
  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),
  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 then a.u_mid else null end)	t1,
  ndv(if(a.isnew='new',a.u_mid,null)) newuv ,
  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),
  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 and a.isnew='new' then a.u_mid else null end) newt1,
  ndv(if(a.isnew='old',a.u_mid,null)) olduv,
  ndv(case when  datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),
  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1  and a.isnew='old' then a.u_mid else null end) noldt1
  from dl.umid_pv a
  left outer join
  dl.umid_pv b on a.u_mid=b.u_mid		
  where  a.dt>='",ldateend.str,"' group by a.dt")

survival = read_data_impala_general(survival_sql)
survival = survival[order(survival$dt),]
survival = survival[-nrow(survival),]
survival$fdt = as.factor(survival$dt)
par(bg = 'black')
colors = rainbow(3)
#考虑survival,!!须注意最后一天
ylarge = max(max(survival$t1/survival$t),max(survival$newt1/survival$newuv),max(survival$noldt1/survival$olduv))
matrix = as.matrix(cbind(newsurvival = survival$newt1/survival$newuv,oldsurvival = survival$noldt1/survival$olduv, totalsurvival = survival$t1/survival$t))
matrix = t(matrix)
#考虑人均pv
par(bg = 'white')
colors <- rainbow(3)
dates <- survival$dt
stacks <- colors
# Create the bar chart.
barplot(matrix,main=paste0(activity_name,"留存率"),names.arg=dates,xlab="date",ylab="留存率",col=colors,beside = TRUE)
legend('topright',c("新用户留存","老用户留存","总体留存"),cex = 0.7,fill = colors,text.col= 'black')

```

次日留存率，新用户并没有明显的波动，几乎保持水平，整体的波动几乎全部来源于老用户，老用户波动也很小，11月8日留存率有个小小的下降趋势

##新老用户访问平均深度（按人头）
```{r depth_person_date}
depth_p_sql = paste0("select t.isnew,t.dt,sum(cast(u_depth as INT))/count(t.u_mid) avg_depth, count(t.u_mid) p_num from 
(select a.dt,isnew,max(b.depth) as u_depth,a.u_mid from 
dm.dm_app_umid_step a  
left outer join 
test.pagelevel b on a.page_name_zh=b.page_name where length(b.depth)=1 and dt >=  '",ldateend.str,"' group by a.dt,a.isnew,a.u_mid) t group by t.isnew,t.dt")
depth_p = read_data_impala_general(depth_p_sql)
depth_p = depth_p[order(depth_p$dt),]
p  = ggplot(depth_p,aes(dt,avg_depth))
p + geom_bar(aes(fill = isnew),stat = "identity",position = "dodge") + labs(x = "日期",y = "平均深度",title = "平均访问深度（按用户）")
```

按人头得出的访问平均深度上，新用户接近超过2，老用户超过2.5
双十一当天用户访问深度最高

##新用户不同机型分布
```{r model,echo = FALSE}
new_user_by_model_date_city_sql = 
paste0("select count(b.u_mid) numbers,firstdt,d_model,l_city from
(select f.u_mid,substr(firstonlinetime,1,10) firstdt,a.d_model,a.l_city from dl.umid_firstonlinetime f left join
  (select * from 
  (select u_mid,d_model,l_city,ROW_NUMBER() OVER (PARTITION BY u_mid,d_model,l_city ORDER BY dt) 
       as level from ods.ods_app_pageview_info where dt>=",ldatestart.str,") t where t.level = 1) a
  using(u_mid) where substr(firstonlinetime,1,10) >= '",as.character(ldatestart),"' 
  and f.dt = '",dateend.str,"') b group by firstdt,d_model,l_city")
new_user_by_model_date_city = read_data_impala_general(new_user_by_model_date_city_sql)
new_user_by_model_date_city = data.table(new_user_by_model_date_city)
lnew_user_by_model_date_city = new_user_by_model_date_city[firstdt < as.character(datestart),]
new_user_by_model_date_city = new_user_by_model_date_city[firstdt >= as.character(datestart),]
new_user_by_model_date = new_user_by_model_date_city[d_model!="null" & str_trim(d_model)!="",.(numbers = sum(numbers)),by = c("d_model","firstdt")]
new_user_by_model = new_user_by_model_date_city[d_model!="null" & str_trim(d_model)!="",.(numbers = sum(numbers)),by = "d_model"][order(numbers,decreasing = T)]
model_list = new_user_by_model[1:100,]$d_model
new_user_by_model_date_top100 = new_user_by_model_date[d_model %in% model_list,]
new_user_by_model_date_top100$model = factor(new_user_by_model_date_top100$d_model,levels = model_list)
new_user_by_model_date_top100_ordered = new_user_by_model_date_top100[order(firstdt,model)]
new_user_by_model_date_ordered_reshape = reshape(new_user_by_model_date_top100_ordered, idvar = "model", timevar = "firstdt", direction = "wide")
new_user_by_model_date_ordered_reshape = dcast(new_user_by_model_date_top100_ordered, model ~ firstdt, value.var = "numbers")
new_user_by_model_date_ordered_reshape$sum = rowSums(new_user_by_model_date_ordered_reshape[,-1],na.rm = T)
melt_df_model_top20 = melt(new_user_by_model_date_ordered_reshape[1:20,-ncol(new_user_by_model_date_ordered_reshape)], id.vars = "model", measure.vars = 2:(ncol(new_user_by_model_date_ordered_reshape)-1), variable.name = "date", value.name = "value")
lmodel_rank = lnew_user_by_model_date_city[d_model!="null" & str_trim(d_model)!="",.(lsum = sum(numbers)),by = c("d_model")]
lmodel_rank$lrank = frank(lmodel_rank,-lsum,ties.method = "min")
new_user_by_model_date_ordered_reshape$rank = seq(new_user_by_model_date_ordered_reshape$sum)
new_user_by_model_date_ordered_reshape = merge(new_user_by_model_date_ordered_reshape,lmodel_rank,by.x = "model",by.y= "d_model",all.x = TRUE)
new_user_by_model_date_ordered_reshape = new_user_by_model_date_ordered_reshape[order(new_user_by_model_date_ordered_reshape$rank),]
rankchange = new_user_by_model_date_ordered_reshape$lrank - new_user_by_model_date_ordered_reshape$rank
symrankchange = ifelse(is.na(rankchange)|rankchange==0,"",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))
sym = paste0(rankchange,symrankchange)
new_user_by_model_date_ordered_reshape$lsum = NULL
new_user_by_model_date_ordered_reshape$lrank = NULL
new_user_by_model_date_ordered_reshape = cbind(sym,new_user_by_model_date_ordered_reshape)
row.names(new_user_by_model_date_ordered_reshape) = new_user_by_model_date_ordered_reshape$rank
DT::datatable(new_user_by_model_date_ordered_reshape, options = list(pageLength = 20))
```

##新用户不同城市分布
```{r city,echo = FALSE}
new_user_by_model_date_city$l_city = str_replace(new_user_by_model_date_city$l_city,"市","")
new_user_by_city_dt = new_user_by_model_date_city[!l_city %in% c("null","局域网","未知"),.(numbers = sum(numbers)),by = c("l_city","firstdt")]
new_user_by_city = new_user_by_model_date_city[!l_city %in% c("null","局域网","未知"),.(numbers = sum(numbers)),by = "l_city"][order(numbers,decreasing = T),]
city_list = new_user_by_city[1:101,]$l_city
new_user_by_city_date_top100 = new_user_by_city_dt[l_city %in% city_list,]
new_user_by_city_date_top100$city = factor(new_user_by_city_date_top100$l_city,levels = city_list)
new_user_by_city_date_top100_ordered = new_user_by_city_date_top100[order(firstdt,city)]
new_user_by_city_date_ordered_reshape = dcast(new_user_by_city_date_top100_ordered, city ~ firstdt, value.var = "numbers")
new_user_by_city_date_ordered_reshape$sum = rowSums(new_user_by_city_date_ordered_reshape[,c(-1,-14)],na.rm = T)
melt_df_city_top20 = melt(new_user_by_city_date_ordered_reshape[1:20,-ncol(new_user_by_city_date_ordered_reshape)], id.vars = "city", measure.vars = 2:(ncol(new_user_by_city_date_ordered_reshape)-1), variable.name = "date", value.name = "value")
lnew_user_by_model_date_city$l_city = str_replace(lnew_user_by_model_date_city$l_city,"市","")
lcity_rank = lnew_user_by_model_date_city[!l_city %in% c("null","局域网","未知"),.(lsum = sum(numbers)),by = c("l_city")]
lcity_rank$lrank = frank(lcity_rank,-lsum,ties.method = "min")
new_user_by_city_date_ordered_reshape$rank = seq(new_user_by_city_date_ordered_reshape$sum)
new_user_by_city_date_ordered_reshape = merge(new_user_by_city_date_ordered_reshape,lcity_rank,by.x = "city",by.y= "l_city",all.x = TRUE)
new_user_by_city_date_ordered_reshape = new_user_by_city_date_ordered_reshape[order(new_user_by_city_date_ordered_reshape$rank),]
rankchange = new_user_by_city_date_ordered_reshape$lrank - new_user_by_city_date_ordered_reshape$rank
symrankchange = ifelse(is.na(rankchange)|rankchange==0,"",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))
sym = paste0(rankchange,symrankchange)
new_user_by_city_date_ordered_reshape$lsum = NULL
new_user_by_city_date_ordered_reshape$lrank = NULL
new_user_by_city_date_ordered_reshape = cbind(sym,new_user_by_city_date_ordered_reshape)
DT::datatable(new_user_by_city_date_ordered_reshape, options = list(pageLength = 20))
```

##新用户不同渠道分布
```{r channel,echo = FALSE}
channel_sql = paste0("select * from dl.dl_channel_umid_openid where substr(umid_firstonlinetime,1,10) >= '",as.character(datestart),"' and dt = ",dateend.str)
new_user_channel_unique = read_data_impala_general(channel_sql)
colnames(new_user_channel_unique) = str_replace(colnames(new_user_channel_unique),"dl_channel_umid_openid.","")
new_user_channel_unique = data.table(new_user_channel_unique)
new_user_channel_unique$date = str_sub(new_user_channel_unique$umid_firstonlinetime,1,10)
new_user_by_channel_date = new_user_channel_unique[,.(numbers = .N),by = c("channel","date")]
new_user_by_channel = new_user_channel_unique[,.(numbers = .N),by = c("channel")][order(numbers,decreasing = T)]
channel_list = new_user_by_channel$channel
new_user_by_channel_date_top = new_user_by_channel_date
new_user_by_channel_date_top$channels = factor(new_user_by_channel_date_top$channel,levels = channel_list)
new_user_by_channel_date_top_ordered = new_user_by_channel_date_top[order(date,channels)]
new_user_by_channel_date_ordered_reshape = dcast(new_user_by_channel_date_top_ordered, channels ~ date, value.var = "numbers")
new_user_by_channel_date_ordered_reshape$sum = rowSums(new_user_by_channel_date_ordered_reshape[,c(-1)],na.rm = T)
DT::datatable(new_user_by_channel_date_ordered_reshape, options = list(pageLength = 20))
#kable(channel,format = "markdown")
```


#转化问题
##每日订单数
```{r order}
online_order_sql = paste0("select count(distinct purchaser_id) as o_num,substr(b.create_date,1,10) as dt from
(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o inner join ods.ods_app_pageview_info a on 
                          o.purchaser_id = a.u_id where substr(o.create_date,1,10)>= '",ldateend,"' and o.order_status not in (1,7,19) and o.order_type=1) b
                          group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)")
online_order = read_data_impala_general(online_order_sql)
total_order_sql = paste0("select count(distinct purchaser_id) as t_num,substr(b.create_date,1,10) as dt from
(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o where substr(o.create_date,1,10)>= '",ldateend,"' and o.order_status not in (1,7,19) and o.order_type=1) b
                         group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)")
total_order = read_data_impala_general(total_order_sql)
matrix = as.matrix(cbind(online_num = online_order$o_num,total_num = total_order$t_num))
matrix = t(matrix)
par(bg = 'white')
colors <- c("blue","purple")
dates <- online_order$dt
stacks <- c("有过APP活动订单数","总订单数")
# Create the bar chart.
barplot(matrix,main="每日订单数",names.arg=dates,xlab="month",ylab="订单数",col=colors,beside = TRUE)
# Add the legend to the chart.
legend("topright",stacks, cex=0.7, fill=colors,xjust = 0, yjust = 1)
```

##家装预约
```{r book}
online_book_sql = paste0("select substr(d.create_date,1,10) dt,count(distinct d.openid) b_num from (select b.create_date,c.openid from
                   ods.ods_jz_business_jz_activity_user_dt b inner join 
ods.ods_db_user_center_users_dt c on b.user_mobile=c.mobile
where b.is_del=0 and substr(create_date,1,10)>='",ldateend,"') d
group by substr(d.create_date,1,10) order by substr(d.create_date,1,10)")
online_book = read_data_impala_general(online_book_sql)
barplot(online_book$b_num,names.arg = online_book$dt,col = "brown",xlab = "month",ylab = "预约数",main = "每日家装预约")
```

##领券情况
```{r coupon}
coupon_sql = paste0("select substr(d.create_time,1,10) dt,count(distinct d.open_id) c_num 
from (select create_time,open_id from ods.ods_marketing_center_mmc_user_coupon_dt where 
to_date(create_time)>='",ldateend,"'
and channel_id not in (2,4) and open_id!='') d
group by substr(d.create_time,1,10) order by substr(d.create_time,1,10)")
coupon = read_data_impala_general(coupon_sql)
coupon = coupon[-nrow(coupon),]
barplot(coupon$c_num,names.arg = coupon$dt,xlab = "month",ylab = "领券数",main = "每日领券情况",col = "blue")
```


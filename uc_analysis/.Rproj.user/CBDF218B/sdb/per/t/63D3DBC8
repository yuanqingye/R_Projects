{
    "collab_server" : "",
    "contents" : "library(ggplot2)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(plyr)\nlibrary(treemap)\nsource('~/Rfile/R_impala.R')\ndateend = Sys.Date()-1\ndatestart = Sys.Date()-2\ndatestart.str = format(datestart,'%Y%m%d')\ndateend.str = format(dateend,'%Y%m%d')\ndates = datestart.str:dateend.str\nldateend = Sys.Date()-3\nldatestart = Sys.Date()-7\nldatestart.str = format(ldatestart,'%Y%m%d')\nldateend.str = format(ldateend,'%Y%m%d')\nactivity_name = \"双十一\"\ncompare_period = \"近一周\"\noptions(scipen = 10)\n\npvuv_sql = paste0(\"select dt,\n                 count(1) uv,\n                 sum(pv) pv ,\n                 sum(pv)/count(1) perpv,\n                 count(if(isnew='new',1,null)) newuv ,\n                 sum(if(isnew='new',pv,0)) newpv, \n                 sum(if(isnew='new',pv,0))/count(if(isnew='new',1,null)) newperpv,\n                 count(if(isnew='old',1,null)) olduv ,\n                 sum(if(isnew='old',pv,0)) oldpv, \n                 sum(if(isnew='old',pv,0))/count(if(isnew='old',1,null)) oldperpv\n                 from \n                 dl.umid_pv where dt>='\",datestart.str,\n                 \"'group by dt\")\n\npvuv = read_data_impala_general(pvuv_sql)\npvuv = pvuv[order(pvuv$dt),]\npvuv$fdt = as.factor(pvuv$dt)\nmatrix = as.matrix(cbind(newuv = pvuv$newuv,olduv = pvuv$olduv))\nmatrix = t(matrix)\nmaxy = max(pvuv$uv)\npar(bg = 'white')\ncolors <- c(\"green\",\"red\")\ndates <- pvuv$dt\nstacks <- c(\"New\",\"Old\")\n# Create the bar chart.\nbarplot(matrix,main=\"Every day uv\",names.arg=dates,xlab=\"month\",ylab=\"uv\",col=colors)\n# Add the legend to the chart.\nlegend(x = 0,y = maxy, stacks, cex=0.7, fill=colors,xjust = 0, yjust = 1)\n\n\npar(bg = 'black')\ncolors = rainbow(3)\nylarge = max(max(pvuv$perpv),max(pvuv$newperpv),max(pvuv$oldperpv))\n#考虑人均pv\nplot(as.numeric(pvuv$fdt),pvuv$perpv,col = colors[[1]],type = 'l',ylim = c(0,ceiling(ylarge)))\nlines(as.numeric(pvuv$fdt),pvuv$newperpv,col = colors[[2]],type = 'l')\nlines(as.numeric(pvuv$fdt),pvuv$oldperpv,col = colors[[3]], type = 'l')\nlegend('topright',c(\"perpv\",\"newperpv\",\"oldperpv\"),cex = 0.7,fill = colors,text.col= 'pink')\n#需要加入坐标轴\ntitle(main = '近两周人均pv',xlab = 'dates',ylab = '人均pv',col.main = 'blue',\n      col.lab = 'purple')\naxis(1,col = 'purple')\nat = str_sub(pvuv$dt,5,8)[seq(from = 2,to = 14,by = 2)]\nmtext(side = 1, text = at, at = seq(from = 2,to = 14,by = 2), col = \"purple\", line = 1)\n\naxis(2,col = 'purple')\nat = axTicks(2)\nmtext(side = 2,text = at,at = at,col = 'purple',line = 1)\n\n#画中画看趋势\nmpvuv_sql = paste0(\"select dt,\n                  count(1) uv,\n                  sum(pv) pv ,\n                  sum(pv)/count(1) perpv,\n                  count(if(isnew='new',1,null)) newuv ,\n                  sum(if(isnew='new',pv,0)) newpv, \n                  sum(if(isnew='new',pv,0))/count(if(isnew='new',1,null)) newperpv,\n                  count(if(isnew='old',1,null)) olduv ,\n                  sum(if(isnew='old',pv,0)) oldpv, \n                  sum(if(isnew='old',pv,0))/count(if(isnew='old',1,null)) oldperpv\n                  from \n                  dl.umid_pv where dt>='\",ldatestart.str,\n                  \"'group by dt\")\nmpvuv = read_data_impala_general(mpvuv_sql)\nmpvuv = mpvuv[order(mpvuv$dt),]\nxlarge = nrow(mpvuv)\nxmid = floor(xlarge/2+0.5)+1\nwpvuv = mpvuv[xmid:xlarge,]\npm <- ggplot(wpvuv, aes(as.Date(dt,format = \"%Y%m%d\"), pv)) + labs(x = \"DATE\",y = \"PV\") + ylim(0,max(wpvuv$pv,na.rm = T))\nmainplot <- pm + geom_line(colour = I(\"purple\")) + labs(title = \"pv trend compare to last 1 month\")\nmainplot\n# p1 <- p + geom_rect(aes(xmin = as.Date(pvuv$dt[11],format = \"%Y%m%d\"), xmax = as.Date(pvuv$dt[14],format = \"%Y%m%d\"),\n#                         ymin = min(pvuv$pv, na.rm = TRUE), ymax = (max(pvuv$pv, na.rm = TRUE)+min(pvuv$pv, na.rm = TRUE))/2),\n#                     fill = alpha(\"lightblue\", 0.2)) + scale_x_continuous(breaks = NA) +\n#   scale_y_continuous(breaks = NA) + labs(y = NULL)\np = ggplot(mpvuv,aes(as.Date(dt,format = \"%Y%m%d\"), pv))+labs(x = \"DATE\",y = \"PV\") + ylim(min(mpvuv$pv,na.rm = T),max(mpvuv$pv,na.rm = T))\np1 <- p + geom_rect(aes(xmin = as.Date(mpvuv$dt[xmid],format = \"%Y%m%d\"), xmax = as.Date(mpvuv$dt[xlarge],format = \"%Y%m%d\"),\n                        ymin = min(mpvuv$pv, na.rm = TRUE), ymax = max(mpvuv$pv, na.rm = TRUE)),\n                    fill = alpha(\"lightblue\", 0.2))\n  # opts(title = \"Full data: 1880-2008\") +\n  # opts(plot.title = theme_text(face = \"bold\")) +\n  # opts(panel.border = theme_blank())\nsubplot <- p1 + geom_line(colour = I(\"grey\"),size = 0.8) \nlibrary(grid)\nvp <- viewport(width = 0.4, height = 0.4, x = 1,\n               y = unit(0.7, \"lines\"), just = c(\"right\",\"bottom\"))\nfull <- function() {\n  print(mainplot)\n  print(subplot, vp = vp)\n}\nfull()\n\n#用户时长\ntime_span_sql = paste0(\"select dt,\n  avg(persvg) totalavg,\n  avg(case when isnew ='new' then persvg else null end) newavg,\n  avg(case when isnew ='old' then persvg else null end) oldavg\nfrom\n(\n  select a.dt,a.u_mid,\n  sum(CAST(p_stay_time AS INT))/1000/60 persvg,\n  case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end isnew \n  from \n  ods.ods_app_pageview_info a \n  left outer join \n  dl.umid_firstonlinetime b on a.u_mid=b.u_mid \n  where a.dt>=\",datestart.str,\" and b.dt='\",dateend.str,\"' and\n  p_domain='mmall.com' and service like '%staytime%' and substr(a.u_mid,1,2)!='a_' and path='z'  and l_city!='测试'\n  and p_type not in ('page.closedown','page.wakeup','page.activate.main') and length(p_stay_time)<=7\n  group by a.dt,a.u_mid,case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end \n)a group by dt\")\n  time_span = read_data_impala_general(time_span_sql)\n  time_span = time_span[order(time_span$dt),]\n  # write.xlsx(time_span,\"~/data/uc_analysis/time_span.xlsx\")\n  time_span$fdt = as.factor(time_span$dt)\n  time_span.new = time_span[,-1]\n  timespan.m <- melt(time_span.new)\n  timespan.m <- ddply(timespan.m, .(variable), transform,rescale = rescale(value))\n  (p <- ggplot(timespan.m, aes(fdt,variable)) + geom_tile(aes(fill = value),colour = \"white\") + scale_fill_gradient(low = \"white\",high = \"purple\"))\n\n  #考虑time span,\n  par(bg = 'white')\n  colors = rainbow(3)\n  ylarge = max(time_span[,2:4])\n  plot(time_span$dt,time_span$totalavg,col = colors[[1]],type = 'l',ylim = c(0,ceiling(ylarge)))\n  lines(time_span$dt,time_span$newavg,col = colors[[2]],type = 'l')\n  lines(time_span$dt,time_span$oldavg,col = colors[[3]], type = 'l')\n  legend('topright',c(\"totalavg\",\"newavg\",\"oldavg\"),cex = 0.7,fill = colors,text.col= 'pink')\n  #需要加入坐标轴\n  title(main = '近两周用户活跃时长',xlab = 'dates',ylab = '',col.main = 'blue',\n        col.lab = 'purple')\n  axis(1,col = 'purple')\n  at = axTicks(1)\n  mtext(side = 1,text = at,at = at,col = \"purple\",line = 1)\n  axis(2,col = 'purple')\n  at = axTicks(2)\n  mtext(side = 2,text = at,at = at,col = 'purple',line = 1)\n\n\n#2persistency/stickness survival\n  survival_sql = paste0(\"select \n  a.dt,ndv(a.u_mid) t,\n  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 then a.u_mid else null end)\tt1,\n  ndv(if(a.isnew='new',a.u_mid,null)) newuv ,\n  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 and a.isnew='new' then a.u_mid else null end) newt1,\n  ndv(if(a.isnew='old',a.u_mid,null)) olduv,\n  ndv(case when  datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1  and a.isnew='old' then a.u_mid else null end) noldt1\n  from dl.umid_pv a\n  left outer join\n  dl.umid_pv b on a.u_mid=b.u_mid\t\t\n  where  a.dt>='\",datestart.str,\"' group by a.dt\")\n\nsurvival = read_data_impala_general(survival_sql)\nsurvival = survival[order(survival$dt),]\nsurvival = survival[-nrow(survival),]\npar(bg = 'black')\ncolors = rainbow(3)\n#考虑survival,!!须注意最后一天\nylarge = max(max(survival$t1/survival$t),max(survival$newt1/survival$newuv),max(survival$noldt1/survival$olduv))\nplot(survival$dt,survival$t1/survival$t,col = colors[[1]],type = 'l',ylim = c(0,ceiling(ylarge)))\nlines(survival$dt,survival$newt1/survival$newuv,col = colors[[2]],type = 'l')\nlines(survival$dt,survival$noldt1/survival$olduv,col = colors[[3]], type = 'l')\nlegend('topright',c(\"survival\",\"newsurvival\",\"oldsurvival\"),cex = 0.7,fill = colors,text.col= 'pink')\n#需要加入坐标轴\ntitle(main = '近两周次日留存率',xlab = 'dates',ylab = '留存率',col.main = 'blue',\n      col.lab = 'purple')\naxis(1,col = 'purple')\nat = axTicks(1)\nmtext(side = 1, text = at, at = at, col = \"purple\", line = 1)\naxis(2,col = 'purple')\nat = axTicks(2)\nmtext(side = 2,text = at,at = at,col = 'purple',line = 1)\n\n# umid_lujing_split_sql = \"select * from test.umid_lujing_split\"\n# umid_lujing_split  = read_data_impala_general(umid_lujing_split_sql)\n# setDT(umid_lujing_split)\n# setnames(umid_lujing_split,\"_c3\",\"path\")\n# umid_lujing_split[,splitted_path:=str_split(path,\"->\")]\n# temp_first_page = sapply(umid_lujing_split$splitted_path,`[[`,1)\n# path_length = sapply(umid_lujing_split$splitted_path,length)\n# temp_last_page = mapply(`[[`,umid_lujing_split$splitted_path,path_length)\n# umid_lujing_split$first_page = temp_first_page\n# umid_lujing_split$last_page = temp_last_page\n# umid_lujing_split$path_length = path_length\n# freq_first_page = umid_lujing_split[,.N,by=c(\"isnew\",\"first_page\")]\n# freq_first_page = freq_first_page[order(isnew,N,decreasing = TRUE),]\n# freq_last_page = umid_lujing_split[,.N,by=c(\"isnew\",\"last_page\")]\n# freq_last_page = freq_last_page[order(isnew,N,decreasing = TRUE),]\n\nlibrary(tidyr)\n# seperated_pages = strsplit(umid_lujing_split$path,\"->\")\n# sep_pages_df = data.frame(seq = rep(1:length(seperated_pages),sapply(seperated_pages,length)),page = unlist(seperated_pages))\n# table_basis2 = readxl::read_xlsx(\"~/data/table_basis.xlsx\")\n# table_basis2 = as.data.frame(table_basis2)\n# sep_pages_df = merge(sep_pages_df,table_basis2,all.x = TRUE,by.x = \"page\",by.y = \"page_name\")\n# sep_pages_df = sep_pages_df[order(sep_pages_df$seq),]\n# depth_result = aggregate(depth~seq, data = sep_pages_df, FUN = max)\n# depth_avg = aggregate(depth~seq,data = sep_pages_df, FUN = mean )\n# umid_lujing_split$seq = 1:nrow(umid_lujing_split)\n# umid_lujing_split = merge(umid_lujing_split,depth_result,by = \"seq\",all.x = TRUE) \n# avg_depth = aggregate(depth~isnew,data = umid_lujing_split,FUN = mean)\n# avg_length = aggregate(path_length~isnew,data = umid_lujing_split,FUN = mean)\n\n\n#1daily activity depth\ndepth_sql = paste0(\"select a.dt,isnew,a.p_channel,b.depth,count(1) from \ndm.dm_app_umid_step a  \nleft outer join \ntest.pagelevel b on a.page_name_zh=b.page_name where length(b.depth)=1 and a.dt>'\",datestart.str,\"' group by a.dt,isnew,b.depth,a.p_channel\")\ndepth = read_data_impala_general(depth_sql)\nsetDT(depth)\ndepth$depth = as.numeric(depth$depth)\navg_depth2 = depth[,.(avg_depth = sum(depth*expr_0)/sum(expr_0)),by = c(\"isnew\",\"dt\")][order(dt,isnew)]\navg_depth_dcast = dcast(avg_depth2,dt~isnew)\nymax = max(max(avg_depth_dcast$new),max(avg_depth_dcast$old))\ndepth_dates = avg_depth_dcast$dt\navg_depth_pic = avg_depth_dcast[,c(2,3)]\nbarplot(t(as.matrix(avg_depth_pic)),beside = TRUE,col = c(\"green\",\"brown4\"),axes = FALSE,ylim=c(0,ymax + 0.6))\naxis(2)\naxis(1,at=seq(from = 2,to = 38,length.out = length(depth_dates)),labels = depth_dates,tick=FALSE)\nlegend('topright',c(\"new user\",\"old user\"),cex = 0.7,fill = c(\"green\",\"brown4\"),text.col= 'blue')\ntitle(main = '近两周新老用户访问平均深度',xlab = 'dates',ylab = '深度',col.main = 'blue',\n      col.lab = 'purple')\n\navg_depth_total = depth[,.(avg_depth = sum(depth*expr_0)/sum(expr_0)),by = c(\"isnew\")]\nbarplot(t(as.matrix(avg_depth_total[,-1])),beside = TRUE,col = c(\"brown4\",\"green\"),axes = FALSE,xlim=c(0,ymax + 0.6),horiz = TRUE)\nlegend('topright',c(\"new user\",\"old user\"),cex = 0.7,fill = c(\"green\",\"brown4\"),text.col= 'blue')\naxis(1)\naxis(2,at=c(1.5,3.5),labels = c(\"old\",\"new\"),tick = TRUE)\ntitle(main = '近两周新老用户访问平均深度',xlab = '深度',ylab = '新老用户',col.main = 'blue',\n      col.lab = 'purple')\n\n#daily activity depth by person\ndepth_p_sql = paste0(\"select t.isnew,t.dt,sum(cast(u_depth as INT))/count(t.u_mid) avg_depth from \n(select a.dt,isnew,max(b.depth) as u_depth,a.u_mid from \ndm.dm_app_umid_step a  \nleft outer join \ntest.pagelevel b on a.page_name_zh=b.page_name where length(b.depth)=1 and dt > '\",datestart.str,\"' group by a.dt,a.isnew,a.u_mid) t group by t.isnew,t.dt\")\ndepth_p = read_data_impala_general(depth_p_sql)\ndepth_p = depth_p[order(depth_p$dt),]\np  = ggplot(depth_p,aes(dt,avg_depth))\np + geom_bar(aes(fill = isnew),stat = \"identity\",position = \"dodge\")\n\nim_sql = paste0(\"select stat_date dt,split_part(send,'_',2) openid, 'im' type from \ndm.im_info a where split_part(send,'_',1)='1' and stat_date>='\",datestart.str,\"' and split_part(rec,'_',1)='2'\")\nim = read_data_impala_general(im_sql)\n\nnew_user_by_model_date_city_sql = \npaste0(\"select count(b.u_mid) numbers,firstdt,d_model,l_city from\n(select f.u_mid,substr(firstonlinetime,1,10) firstdt,a.d_model,a.l_city from dl.umid_firstonlinetime f left join\n  (select * from \n  (select u_mid,d_model,l_city,ROW_NUMBER() OVER (PARTITION BY u_mid,d_model,l_city ORDER BY dt) \n       as level from ods.ods_app_pageview_info where dt>=\",ldatestart.str,\") t where t.level = 1) a\n  using(u_mid) where substr(firstonlinetime,1,10) >= '\",as.character(ldatestart),\"' \n  and f.dt = '\",dateend.str,\"') b group by firstdt,d_model,l_city\")\nnew_user_by_model_date_city = read_data_impala_general(new_user_by_model_date_city_sql)\nnew_user_by_model_date_city = data.table(new_user_by_model_date_city)\nlnew_user_by_model_date_city = new_user_by_model_date_city[firstdt < as.character(datestart),]\nnew_user_by_model_date_city = new_user_by_model_date_city[firstdt >= as.character(datestart),]\nnew_user_by_model_date = new_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(numbers = sum(numbers)),by = c(\"d_model\",\"firstdt\")]\nnew_user_by_model = new_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(numbers = sum(numbers)),by = \"d_model\"][order(numbers,decreasing = T)]\nmodel_list = new_user_by_model[1:100,]$d_model\nnew_user_by_model_date_top100 = new_user_by_model_date[d_model %in% model_list,]\nnew_user_by_model_date_top100$model = factor(new_user_by_model_date_top100$d_model,levels = model_list)\nnew_user_by_model_date_top100_ordered = new_user_by_model_date_top100[order(firstdt,model)]\nnew_user_by_model_date_ordered_reshape = reshape(new_user_by_model_date_top100_ordered, idvar = \"model\", timevar = \"firstdt\", direction = \"wide\")\nnew_user_by_model_date_ordered_reshape = dcast(new_user_by_model_date_top100_ordered, model ~ firstdt, value.var = \"numbers\")\nnew_user_by_model_date_ordered_reshape$sum = rowSums(new_user_by_model_date_ordered_reshape[,-1],na.rm = T)\nmelt_df_model_top20 = melt(new_user_by_model_date_ordered_reshape[1:20,-ncol(new_user_by_model_date_ordered_reshape)], id.vars = \"model\", measure.vars = 2:(ncol(new_user_by_model_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\nlmodel_rank = lnew_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(lsum = sum(numbers)),by = c(\"d_model\")]\nlmodel_rank$lrank = frank(lmodel_rank,-lsum,ties.method = \"min\")\nnew_user_by_model_date_ordered_reshape$rank = seq(new_user_by_model_date_ordered_reshape$sum)\nnew_user_by_model_date_ordered_reshape = merge(new_user_by_model_date_ordered_reshape,lmodel_rank,by.x = \"model\",by.y= \"d_model\",all.x = TRUE)\nnew_user_by_model_date_ordered_reshape = new_user_by_model_date_ordered_reshape[order(new_user_by_model_date_ordered_reshape$rank),]\nrankchange = new_user_by_model_date_ordered_reshape$lrank - new_user_by_model_date_ordered_reshape$rank\nsymrankchange = ifelse(is.na(rankchange)|rankchange==0,\"\",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))\nsym = paste0(rankchange,symrankchange)\nnew_user_by_model_date_ordered_reshape$lsum = NULL\nnew_user_by_model_date_ordered_reshape$lrank = NULL\nnew_user_by_model_date_ordered_reshape = cbind(sym,new_user_by_model_date_ordered_reshape)\nrow.names(new_user_by_model_date_ordered_reshape) = new_user_by_model_date_ordered_reshape$rank\nDT::datatable(new_user_by_model_date_ordered_reshape, options = list(pageLength = 20))\n\n# melt_df_model_top20 = melt(new_user_by_model_date_ordered_reshape[1:20,-ncol(new_user_by_model_date_ordered_reshape)], id.vars = \"model\", measure.vars = 2:(ncol(new_user_by_model_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\ntreemap(melt_df_model_top20,index = c(\"date\",\"model\"),vSize = \"value\")\n\nnew_user_by_model_date_city$l_city = str_replace(new_user_by_model_date_city$l_city,\"市\",\"\")\nnew_user_by_city_dt = new_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(numbers = sum(numbers)),by = c(\"l_city\",\"firstdt\")]\nnew_user_by_city = new_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(numbers = sum(numbers)),by = \"l_city\"][order(numbers,decreasing = T),]\ncity_list = new_user_by_city[1:101,]$l_city\nnew_user_by_city_date_top100 = new_user_by_city_dt[l_city %in% city_list,]\nnew_user_by_city_date_top100$city = factor(new_user_by_city_date_top100$l_city,levels = city_list)\nnew_user_by_city_date_top100_ordered = new_user_by_city_date_top100[order(firstdt,city)]\nnew_user_by_city_date_ordered_reshape = dcast(new_user_by_city_date_top100_ordered, city ~ firstdt, value.var = \"numbers\")\nnew_user_by_city_date_ordered_reshape$sum = rowSums(new_user_by_city_date_ordered_reshape[,c(-1,-14)],na.rm = T)\nnew_user_by_city_date_ordered_reshape$city = as.character(new_user_by_city_date_ordered_reshape$city)\nmelt_df_city_top20 = melt(new_user_by_city_date_ordered_reshape[1:20,-ncol(new_user_by_city_date_ordered_reshape)], id.vars = \"city\", measure.vars = 2:(ncol(new_user_by_city_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\nlnew_user_by_model_date_city$l_city = str_replace(lnew_user_by_model_date_city$l_city,\"市\",\"\")\nlcity_rank = lnew_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(lsum = sum(numbers)),by = c(\"l_city\")]\nlcity_rank$lrank = frank(lcity_rank,-lsum,ties.method = \"min\")\nnew_user_by_city_date_ordered_reshape$rank = seq(new_user_by_city_date_ordered_reshape$sum)\nnew_user_by_city_date_ordered_reshape = merge(new_user_by_city_date_ordered_reshape,lcity_rank,by.x = \"city\",by.y= \"l_city\",all.x = TRUE)\nnew_user_by_city_date_ordered_reshape = new_user_by_city_date_ordered_reshape[order(new_user_by_city_date_ordered_reshape$rank),]\nrankchange = new_user_by_city_date_ordered_reshape$lrank - new_user_by_city_date_ordered_reshape$rank\nsymrankchange = ifelse(is.na(rankchange)|rankchange==0,\"\",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))\nsym = paste0(rankchange,symrankchange)\nnew_user_by_city_date_ordered_reshape$lsum = NULL\nnew_user_by_city_date_ordered_reshape$lrank = NULL\nnew_user_by_city_date_ordered_reshape = cbind(sym,new_user_by_city_date_ordered_reshape)\nrow.names(new_user_by_city_date_ordered_reshape) = new_user_by_city_date_ordered_reshape$rank\n\n\n# melt_df_city_top20 = melt(new_user_by_city_date_ordered_reshape[1:20,-ncol(new_user_by_city_date_ordered_reshape)], id.vars = \"city\", measure.vars = 2:(ncol(new_user_by_city_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\ntreemap(melt_df_city_top20,index=c(\"date\",\"city\"),vSize = \"value\")\n\nchannel_sql = paste0(\"select * from dl.dl_channel_umid_openid where substr(umid_firstonlinetime,1,10) > '\",as.character(datestart),\"' and dt = \",dateend.str)\nnew_user_channel_unique = read_data_impala_general(channel_sql)\ncolnames(new_user_channel_unique) = str_replace(colnames(new_user_channel_unique),\"dl_channel_umid_openid.\",\"\")\n# nrow(new_user_channel[!duplicated(new_user_channel$dl_channel_umid_openid.u_mid),])\n# new_user_channel_unique = new_user_channel[!duplicated(new_user_channel$dl_channel_umid_openid.u_mid),]\nnew_user_channel_unique = data.table(new_user_channel_unique)\nnew_user_channel_unique$date = str_sub(new_user_channel_unique$umid_firstonlinetime,1,10)\nnew_user_by_channel_date = new_user_channel_unique[,.(numbers = .N),by = c(\"channel\",\"date\")]\nnew_user_by_channel = new_user_channel_unique[,.(numbers = .N),by = c(\"channel\")][order(numbers,decreasing = T)]\nchannel_list = new_user_by_channel$channel\nnew_user_by_channel_date_top = new_user_by_channel_date\nnew_user_by_channel_date_top$channels = factor(new_user_by_channel_date_top$channel,levels = channel_list)\nnew_user_by_channel_date_top_ordered = new_user_by_channel_date_top[order(date,channels)]\nnew_user_by_channel_date_ordered_reshape = dcast(new_user_by_channel_date_top_ordered, channels ~ date, value.var = \"numbers\")\nnew_user_by_channel_date_ordered_reshape$sum = rowSums(new_user_by_channel_date_ordered_reshape[,c(-1)],na.rm = T)\nmelt_df_channel_top20 = melt(new_user_by_channel_date_ordered_reshape[1:20,-ncol(new_user_by_channel_date_ordered_reshape)], id.vars = \"channels\", measure.vars = 2:(ncol(new_user_by_channel_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\ntreemap(melt_df_channel_top20,index=c(\"date\",\"channels\"),vSize = \"value\")\n\n\nonline_order_sql = paste0(\"select count(distinct purchaser_id) as o_num,substr(b.create_date,1,10) as dt from\n(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o inner join ods.ods_app_pageview_info a on \n                          o.purchaser_id = a.u_id where substr(o.create_date,1,10)> '\",datestart,\"' and o.order_status not in (1,7,19) and o.order_type=1) b\n                          group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)\")\nonline_order = read_data_impala_general(online_order_sql)\n\ntotal_order_sql = paste0(\"select count(distinct purchaser_id) as t_num,substr(b.create_date,1,10) as dt from\n(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o where substr(o.create_date,1,10)> '\",datestart,\"' and o.order_status not in (1,7,19) and o.order_type=1) b\n                         group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)\")\ntotal_order = read_data_impala_general(total_order_sql)\n\nonline_book_sql = paste0(\"select substr(d.create_date,1,10) dt,count(distinct d.openid) b_num from (select b.create_date,c.openid from\n                   ods.ods_jz_business_jz_activity_user_dt b inner join \nods.ods_db_user_center_users_dt c on b.user_mobile=c.mobile\nwhere b.is_del=0 and substr(create_date,1,10)>='\",datestart,\"') d\ngroup by substr(d.create_date,1,10) order by substr(d.create_date,1,10)\")\nonline_book = read_data_impala_general(online_book_sql)\n\ncoupon_sql = paste0(\"select substr(d.create_time,1,10) dt,count(distinct d.open_id) c_num \nfrom (select create_time,open_id from ods.ods_marketing_center_mmc_user_coupon_dt where \nto_date(create_time)>='\",datestart,\"'\nand channel_id not in (2,4) and open_id!='') d\ngroup by substr(d.create_time,1,10) order by substr(d.create_time,1,10)\")\ncoupon = read_data_impala_general(coupon_sql)\n\n\n",
    "created" : 1508912759571.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1098814255",
    "id" : "63D3DBC8",
    "lastKnownWriteTime" : 1510546205,
    "last_content_update" : 1510546205206,
    "path" : "~/R_Projects/uc_analysis/Rfile/User_analysis_topic.R",
    "project_path" : "Rfile/User_analysis_topic.R",
    "properties" : {
        "source_window_id" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 16,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}
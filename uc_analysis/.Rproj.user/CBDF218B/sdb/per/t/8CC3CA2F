{
    "collab_server" : "",
    "contents" : "---\ntitle: \"User_analysis\"\nauthor: \"袁青野\"\noutput: html_document\nalways_allow_html: yes\n---\n\n```{r setup,include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(readxl)\nlibrary(knitr)\nlibrary(xtable)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(plyr)\nlibrary(treemap)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(cowplot)\n\nsource('~/Rfile/R_impala.R')\n# source('~/Rfile/R_hive.R')\ndatestart = Sys.Date()-14\ndateend = Sys.Date()-1\ndatestart.str = format(datestart,'%Y%m%d')\ndateend.str = format(dateend,'%Y%m%d')\ndates = datestart.str:dateend.str\nldateend = Sys.Date()-15\nldatestart = Sys.Date()-28\nldatestart.str = format(ldatestart,'%Y%m%d')\nldateend.str = format(ldateend,'%Y%m%d')\noptions(scipen = 10)\n\n```\n#本报告对`r datestart`用户至`r dateend`这两周的行为进行了分析\n####包括反映用户数目的UV,反映用户活跃度的：人均PV,用户活跃时长,以及反映用户黏度的次日留存，报告还包含了路径深度的分析，转化部分的内容,并采用多种方式作图\n####本次报告用R语言自动化生成报告的框架搭建，今后将在此基础上不断改进优化\n\n#用户黏性\n## 次日留存\n```{r survival,echo=FALSE}\n  survival_sql = paste0(\"select \n  a.dt,ndv(a.u_mid) t,\n  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 then a.u_mid else null end)\tt1,\n  ndv(if(a.isnew='new',a.u_mid,null)) newuv ,\n  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 and a.isnew='new' then a.u_mid else null end) newt1,\n  ndv(if(a.isnew='old',a.u_mid,null)) olduv,\n  ndv(case when  datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1  and a.isnew='old' then a.u_mid else null end) noldt1\n  from dl.umid_pv a\n  left outer join\n  dl.umid_pv b on a.u_mid=b.u_mid\t\t\n  where  a.dt>='\",datestart.str,\"' group by a.dt\")\nsurvival = read_data_impala_general(survival_sql)\n# survival = read_data_hive_general(survival_sql)\nsurvival = survival[order(survival$dt),]\nsurvival = survival[-nrow(survival),]\nsurvival$fdt = as.factor(survival$dt)\npar(bg = 'black')\ncolors = rainbow(3)\n#考虑survival,!!须注意最后一天\nylarge = max(max(survival$t1/survival$t),max(survival$newt1/survival$newuv),max(survival$noldt1/survival$olduv))\nplot(as.numeric(survival$fdt),survival$t1/survival$t,col = colors[[1]],type = 'l',ylim = c(0,ceiling(ylarge)))\nlines(as.numeric(survival$fdt),survival$newt1/survival$newuv,col = colors[[2]],type = 'l')\nlines(as.numeric(survival$fdt),survival$noldt1/survival$olduv,col = colors[[3]], type = 'l')\nlegend('topright',c(\"survival\",\"newsurvival\",\"oldsurvival\"),cex = 0.7,fill = colors,text.col= 'pink')\n#需要加入坐标轴\ntitle(main = '近两周次日留存率',xlab = 'dates',ylab = '留存率',col.main = 'blue',\n      col.lab = 'purple')\naxis(1,col = 'purple')\nat = str_sub(survival$dt,5,8)[seq(from = 2,to = 14,by = 2)]\nmtext(side = 1, text = at, at = seq(from = 2,to = 14,by = 2), col = \"purple\", line = 1)\n\naxis(2,col = 'purple')\nat = axTicks(2)\nmtext(side = 2,text = at,at = at,col = 'purple',line = 1)\n\n```\n\n次日留存率，老用户波动不明显，老用户整体明显高于新用户,新用户在4月14日有个小高峰\n\n#用户活跃\n##近两周的日活跃度(UV)\n```{r daily_pvuv,echo = FALSE}\n#1daily activity pvuv\npvuv_sql = paste0(\"select dt,\n                 count(1) uv,\n                 sum(pv) pv ,\n                 sum(pv)/count(1) perpv,\n                 count(if(isnew='new',1,null)) newuv ,\n                 sum(if(isnew='new',pv,0)) newpv, \n                 sum(if(isnew='new',pv,0))/count(if(isnew='new',1,null)) newperpv,\n                 count(if(isnew='old',1,null)) olduv ,\n                 sum(if(isnew='old',pv,0)) oldpv, \n                 sum(if(isnew='old',pv,0))/count(if(isnew='old',1,null)) oldperpv\n                 from \n                 dl.umid_pv where dt>='\",ldatestart.str,\n                 \"'group by dt\")\nmpvuv = read_data_impala_general(pvuv_sql)\nmpvuv = mpvuv[order(mpvuv$dt),]\nxlarge = nrow(mpvuv)\nxmid = floor(xlarge/2+0.5)+1\npvuv = mpvuv[xmid:xlarge,]\nwpvuv = pvuv\npvuv$fdt = as.factor(pvuv$dt)\nmatrix = as.matrix(cbind(newuv = pvuv$newuv,olduv = pvuv$olduv))\nmatrix = t(matrix)\nmaxy = max(pvuv$uv)\nmaxx = pvuv[which.max(pvuv$uv),\"dt\"]\nmeany = mean(pvuv$uv)\nnewbetterthanold = sum(pvuv$newuv>pvuv$olduv)\nabnormalhighdt = pvuv[pvuv$uv>1.3*meany,\"dt\"]\nabnormallowdt = pvuv[pvuv$uv<0.7*meany,\"dt\"]\npar(bg = 'white')\ncolors <- c(\"green\",\"red\")\ndates <- pvuv$dt\nstacks <- c(\"Old\",\"New\")\nbplot = barplot(matrix,main=\"Every day uv\",names.arg=dates,xlab=\"month\",ylab=\"uv\",col=colors,legend.text = stacks,args.legend = list(x = 0,y = maxy, stacks, cex=0.7, fill=colors,xjust = 0, yjust = 1))\n\n```\n\n平均UV为`r round(meany)`,\n\n最大UV为`r round(maxy)`,\n\n最大值发生在`r maxx`\n\n有`r newbetterthanold` 天新用户在UV值上超过老用户\n\n`r if(length(abnormalhighdt)==0){\"并没有哪天\"} else{abnormalhighdt}` 用户UV明显高于平均值    \n`r if(length(abnormallowdt)==0){\"并没有哪天\"} else{abnormallowdt}` 用户UV明显低于平均值    \n整体uv值呈现不断上升的趋势\n<!-- `r ifelse(length(abnormallowdt)==0,\"并没有哪天\",abnormallowdt)` 用户UV明显低于平均值 -->\n\n##近两周人均PV\n```{r pv_perperson,echo = FALSE}\npar(bg = 'black')\ncolors = rainbow(3)\nylarge = max(max(pvuv$perpv),max(pvuv$newperpv),max(pvuv$oldperpv))\ntmax = max(pvuv$perpv)\ndttmax = pvuv[which.max(pvuv$perpv),\"dt\"]\ntmin = min(pvuv$perpv)\ndttmin = pvuv[which.min(pvuv$perpv),\"dt\"]\ntmean = mean(pvuv$perpv)\n\nnmax = max(pvuv$newperpv)\ndtnmax = pvuv[which.max(pvuv$newperpv),\"dt\"]\nnmin = min(pvuv$newperpv)\ndtnmin = pvuv[which.min(pvuv$newperpv),\"dt\"]\nnmean = mean(pvuv$newperpv)\n\nomax = max(pvuv$oldperpv)\ndtomax = pvuv[which.max(pvuv$oldperpv),\"dt\"]\nomin = min(pvuv$oldperpv)\ndtomin = pvuv[which.min(pvuv$oldperpv),\"dt\"]\nomean = mean(pvuv$oldperpv)\n\nnewbetterthanold = sum(pvuv$newperpv>pvuv$oldperpv)\ndifftrend = pvuv[!(c(1,sign(diff(pvuv$newperpv)*diff(pvuv$oldperpv)))+1),\"dt\"]\n\n#考虑人均pv\nplot(as.numeric(pvuv$fdt),pvuv$perpv,col = colors[[1]],type = 'l',ylim = c(0,ceiling(ylarge)))\nlines(as.numeric(pvuv$fdt),pvuv$newperpv,col = colors[[2]],type = 'l')\nlines(as.numeric(pvuv$fdt),pvuv$oldperpv,col = colors[[3]], type = 'l')\n\nlegend('bottomright',c(\"perpv\",\"newperpv\",\"oldperpv\"),cex = 0.7,fill = colors,text.col= 'pink')\n\n#需要加入坐标轴\ntitle(main = '近两周人均pv',xlab = 'dates',ylab = '人均pv',col.main = 'blue',\n      col.lab = 'purple')\naxis(1,col = 'purple')\nat = str_sub(pvuv$dt,5,8)[seq(from = 2,to = 14,by = 2)]\nmtext(side = 1, text = at, at = seq(from = 2,to = 14,by = 2), col = \"purple\", line = 1)\n\naxis(2,col = 'purple')\nat = axTicks(2)\nmtext(side = 2,text = at,at = at,col = 'purple',line = 1)\n```\n\n总体PV均值为`r tmean` \n\n总体PV最大值为`r tmax` 最大值发生在 `r dttmax`\n\n总体PV最小值为`r tmin` 最小值发生在 `r dttmin`\n\n新用户PV均值为`r nmean` \n\n新用户PV最大值为`r nmax` 最大值发生在 `r dtnmax`\n\n新用户PV最小值为`r nmin` 最小值发生在 `r dtnmin`\n\n老用户PV均值为`r omean` \n\n老用户PV最大值为`r omax` 最大值发生在 `r dtomax`\n\n老用户PV最小值为`r omin` 最小值发生在 `r dtomin`\n\n有`r newbetterthanold` 天新用户在人均PV值上超过老用户\n\n`r difftrend` 新用户和老用户的趋势不一样\n\n```{r pvuv_trend,echo=FALSE,results=\"hide\"} \n##results = hide\nSys.setlocale(category = \"LC_ALL\", locale = \"English_United States.1252\")\npm <- ggplot(wpvuv, aes(as.Date(dt,format = \"%Y%m%d\"), pv)) + labs(x = \"DATE\",y = \"PV\") + ylim(0,max(wpvuv$pv,na.rm = T))\nmainplot <- pm + geom_line(colour = I(\"purple\")) + labs(title = \"pv trend compare to last 1 month\")+scale_x_date(labels = date_format(\"%m-%d\"),date_breaks = \"3 days\")\n# p = ggplot(mpvuv,aes(as.Date(dt,format = \"%Y%m%d\"), pv))+labs(x = \"DATE\",y = \"PV\") + ylim(min(mpvuv$pv,na.rm = T),max(mpvuv$pv,na.rm = T))\np = ggplot(mpvuv,aes(as.Date(dt,format = \"%Y%m%d\"), pv)) + ylim(min(mpvuv$pv,na.rm = T),max(mpvuv$pv,na.rm = T))+scale_x_date(labels = date_format(\"%m-%d\"),date_breaks = \"10 days\")+theme(axis.title = element_blank())\np1 <- p + geom_rect(aes(xmin = as.Date(mpvuv$dt[xmid],format = \"%Y%m%d\"), xmax = as.Date(mpvuv$dt[xlarge],format = \"%Y%m%d\"),\n                        ymin = min(mpvuv$pv, na.rm = TRUE), ymax = max(mpvuv$pv, na.rm = TRUE)),fill = alpha(\"lightblue\", 0.2))\nsubplot <- p1 + geom_line(colour = I(\"grey\"),size = 0.8) \n# vp <- viewport(width = 0.4, height = 0.4, x = 1,\n#                y = unit(0.7, \"lines\"), just = c(\"right\",\"bottom\"))\nvp <- viewport(width = 0.6, height = 0.4, x = 0.75,\n                y = 0.25, just = c(\"right\",\"bottom\"))\nfull <- function() {\n  print(mainplot)\n  print(subplot, vp = vp)\n}\nfull()\n```\n\n画中画分析，*大图全图*和*小图中高亮处*均表示近两周pv趋势，而小图整体表示近一个月的pv情况    \n近两周整体pv出现缓慢不断上升情形，**4月22日（周日）达到顶峰**\n\n## 用户活跃时长\n```{r time_span,echo=FALSE,warning=FALSE,results=\"hide\",message=FALSE}\nSys.setlocale(category = \"LC_ALL\", locale = \"\")\ntime_span_sql = paste0(\"select dt,\n  avg(persvg) totalavg,\n  avg(case when isnew ='new' then persvg else null end) newavg,\n  avg(case when isnew ='old' then persvg else null end) oldavg\nfrom\n(\n  select a.dt,a.u_mid,\n  sum(CAST(p_stay_time AS INT))/1000/60 persvg,\n  case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end isnew \n  from \n  ods.ods_app_pageview_info a \n  left outer join \n  dl.umid_firstonlinetime b on a.u_mid=b.u_mid \n  where a.dt>=\",datestart.str,\" and b.dt='\",dateend.str,\"' and\n  p_domain='mmall.com' and service like '%staytime%' and substr(a.u_mid,1,2)!='a_' and path='z'  and l_city!='测试'\n  and p_type not in ('page.closedown','page.wakeup','page.activate.main') and length(p_stay_time)<=7\n  group by a.dt,a.u_mid,case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end \n)a group by dt\")\n  time_span = read_data_impala_general(time_span_sql)\n  time_span = time_span[order(time_span$dt),]\n  # write.xlsx(time_span,\"~/data/uc_analysis/time_span.xlsx\")\n  time_span$fdt = as.factor(time_span$dt)\n  time_span.new = time_span[,-1]\n  timespan.m <- melt(time_span.new)\n  timespan.m <- ddply(timespan.m, .(variable), transform,rescale = rescale(value))\n  timespan.m$dates = str_sub(as.character(timespan.m$fdt),6,8)\n  timespan.m$dates = factor(timespan.m$dates, levels = timespan.m$dates)\n  names(timespan.m) = c(\"fdt\",\"old_new_user\",\"time_span\",\"rescale\",\"dates\")\n# (p <- ggplot(timespan.m, aes(dates,old_new_user)) + geom_tile(aes(fill = time_span),colour = \"white\") + scale_fill_gradient(low = \"white\",high = \"purple\"))\n  p <- ggplot(timespan.m, aes(dates,old_new_user)) + geom_tile(aes(fill = time_span),colour = \"white\") + scale_fill_gradient(low = \"white\",high = \"purple\")\n  df = as.data.frame(t(time_span.new))\n  colnames(df) = time_span.new$fdt\n  df = df[-nrow(df),]\n  tt <- ttheme_default(colhead=list(fg_params = list(parse=TRUE)))\n  tbl1 <- tableGrob(df[,1:round(ncol(df)/2)], rows=rownames(df), theme=tt)\n  tbl2 <- tableGrob(df[,(round(ncol(df)/2)+1):ncol(df)], rows=rownames(df), theme=tt)\n# Plot chart and table into one object\n  grid.arrange(p, tbl1,tbl2,nrow=3,as.table=TRUE\n             # ,heights=c(3,1)\n             )\n  # plot_grid(p, tbl1, tbl2, align = \"v\", nrow = 3, rel_heights = c(1/2, 1/4, 1/4))\n  # rownames(time_span.new) = time_span.new$fdt\n  # DT::datatable(time_span.new[,-ncol(time_span.new)])\n```\n\n对于用户访问时长作的热力图    \n在本报告所覆盖的两周，新用户访问时长明显长过老用户    \n新用户访问时长在4月13日前较高，在4月18日，19日也出现平均访问时长的高峰，其背后原因，值得深入挖掘\n\n##新老用户访问平均深度（按用户）\n```{r depth_person_date,echo=FALSE,warning = FALSE}\ndepth_p_sql = paste0(\"select t.isnew,t.dt,sum(cast(u_depth as INT))/count(t.u_mid) avg_depth, count(t.u_mid) p_num from \n(select a.dt,isnew,max(b.depth) as u_depth,a.u_mid from \ndm.dm_app_umid_step a  \nleft outer join \ntest.pagelevel2 b on a.page_name_zh=b.page_name where length(b.depth)=1 and a.dt > '\",datestart.str,\"' group by a.dt,a.isnew,a.u_mid) t group by t.isnew,t.dt\")\ndepth_p = read_data_impala_general(depth_p_sql)\ndepth_p = depth_p[order(depth_p$dt),]\ndepth_p$fdt = factor(str_sub(depth_p$dt,5,8),levels = str_sub(depth_p$dt,5,8))\np  = ggplot(depth_p,aes(fdt,avg_depth))\np = p + geom_bar(aes(fill = isnew),stat = \"identity\",position = \"dodge\") + labs(x = \"日期\",y = \"平均深度\",title = \"平均访问深度（按用户）\")\ndepth_p$fdt = NULL\ndepth_p$value = round(depth_p$avg_depth,digits = 2)\ndf = dcast(depth_p[,-4],isnew~dt,value.var = \"value\")\nrownames(df) = df[,1]\ndf = df[,-1]\ntt <- ttheme_default(colhead=list(fg_params = list(parse=TRUE)))\ntb1 = tableGrob(df[,1:round(ncol(df)/2)], rows=rownames(df), theme=tt)\ntb2 = tableGrob(df[,(round(ncol(df)/2)+1):ncol(df)], rows=rownames(df), theme=tt)\ntb_1_2 = arrangeGrob(grobs = list(tb1,tb2),nrow = 2,ncol = 1)\ngrid.arrange(p,tb_1_2,nrow = 2)\n```\n\n新用户访问深度基本高于老用户，新老用户访问深度差距明显，老用户访问深度几乎没有波动，比较令人奇怪    \n\n#新用户注册渠道城市机型数量分析\n##新用户不同机型分布\n```{r model,echo = FALSE}\n# datestart = as.Date(\"2017-12-02\",\"%Y-%m-%d\")\n# dateend = datestart+14\n# datestart.str = format(datestart,'%Y%m%d')\n# dateend.str = format(dateend,'%Y%m%d')\n# ldateend = datestart-1\n# ldatestart = datestart-15\n# ldatestart.str = format(ldatestart,'%Y%m%d')\n# ldateend.str = format(ldateend,'%Y%m%d')\n\nnew_user_by_model_date_city_sql = \npaste0(\"select count(b.u_mid) numbers,firstdt,d_model,l_city from\n(select f.u_mid,substr(firstonlinetime,1,10) firstdt,a.d_model,a.l_city from dl.umid_firstonlinetime f left join\n  (select * from\n  (select u_mid,d_model,l_city,ROW_NUMBER() OVER (PARTITION BY u_mid,d_model,l_city ORDER BY dt) \n       as level from ods.ods_app_pageview_info where dt>=\",ldatestart.str,\") t where t.level = 1) a\n  using(u_mid) where substr(firstonlinetime,1,10) >= '\",as.character(ldatestart),\"' \n  and f.dt = '\",dateend.str,\"') b group by firstdt,d_model,l_city\")\nnew_user_by_model_date_city = read_data_impala_general(new_user_by_model_date_city_sql)\nnew_user_by_model_date_city = data.table(new_user_by_model_date_city)\nlnew_user_by_model_date_city = new_user_by_model_date_city[firstdt < as.character(datestart),]\nnew_user_by_model_date_city = new_user_by_model_date_city[firstdt >= as.character(datestart),]\nnew_user_by_model_date = new_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(numbers = sum(numbers)),by = c(\"d_model\",\"firstdt\")]\nnew_user_by_model = new_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(numbers = sum(numbers)),by = \"d_model\"][order(numbers,decreasing = T)]\nmodel_list = new_user_by_model[1:100,]$d_model\nnew_user_by_model_date_top100 = new_user_by_model_date[d_model %in% model_list,]\nnew_user_by_model_date_top100$model = factor(new_user_by_model_date_top100$d_model,levels = model_list)\nnew_user_by_model_date_top100_ordered = new_user_by_model_date_top100[order(firstdt,model)]\n# new_user_by_model_date_ordered_reshape = reshape(new_user_by_model_date_top100_ordered, idvar = \"model\", timevar = \"firstdt\", direction = \"wide\")\nnew_user_by_model_date_ordered_reshape = dcast(new_user_by_model_date_top100_ordered, model ~ firstdt, value.var = \"numbers\")\nnew_user_by_model_date_ordered_reshape$sum =rowSums(new_user_by_model_date_ordered_reshape[,-1],na.rm = T)\nmelt_df_model_top20 = melt(new_user_by_model_date_ordered_reshape[1:20,-ncol(new_user_by_model_date_ordered_reshape)], id.vars = \"model\", measure.vars = 2:(ncol(new_user_by_model_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\nlmodel_rank = lnew_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(lsum = sum(numbers)),by = c(\"d_model\")]\nlmodel_rank$lrank = frank(lmodel_rank,-lsum,ties.method = \"min\")\nnew_user_by_model_date_ordered_reshape$rank = seq(new_user_by_model_date_ordered_reshape$sum)\nnew_user_by_model_date_ordered_reshape = merge(new_user_by_model_date_ordered_reshape,lmodel_rank,by.x = \"model\",by.y= \"d_model\",all.x = TRUE)\nnew_user_by_model_date_ordered_reshape = new_user_by_model_date_ordered_reshape[order(new_user_by_model_date_ordered_reshape$rank),]\nrankchange = new_user_by_model_date_ordered_reshape$lrank - new_user_by_model_date_ordered_reshape$rank\nsymrankchange = ifelse(is.na(rankchange)|rankchange==0,\"\",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))\nsym = paste0(rankchange,symrankchange)\nnew_user_by_model_date_ordered_reshape$lsum = NULL\nnew_user_by_model_date_ordered_reshape$lrank = NULL\nnew_user_by_model_date_ordered_reshape = cbind(sym,new_user_by_model_date_ordered_reshape)\nrownames(new_user_by_model_date_ordered_reshape) = 1:nrow(new_user_by_model_date_ordered_reshape)\nDT::datatable(new_user_by_model_date_ordered_reshape, options = list(pageLength = 20))\n```\n\niphone手机排名前列，前十名完全被iphone系列手机占据,MI6 排名第11,上升18位,Redmi Note 4X,排名第15,上升11位,OPPO A33,排名第16,上升184位,Mi Note 3,排名第20,上升43位\n整体流量相对以往大促偏低，而数据也显得更为可信\n\n```{r model_treemap}\ntreemap(melt_df_model_top20,index = c(\"date\",\"model\"),vSize = \"value\")\n```\n\n结构矩形树图，大矩形为日期，小矩形为具体机型，按从大到小的顺序，从上到下，从左到右排列\n\n##新用户不同城市分布\n```{r city,echo = FALSE}\nnew_user_by_model_date_city$l_city = str_replace(new_user_by_model_date_city$l_city,\"市\",\"\")\nnew_user_by_city_dt = new_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(numbers = sum(numbers)),by = c(\"l_city\",\"firstdt\")]\nnew_user_by_city = new_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(numbers = sum(numbers)),by = \"l_city\"][order(numbers,decreasing = T),]\ncity_list = new_user_by_city[1:101,]$l_city\nnew_user_by_city_date_top100 = new_user_by_city_dt[l_city %in% city_list,]\nnew_user_by_city_date_top100$city = factor(new_user_by_city_date_top100$l_city,levels = city_list)\nnew_user_by_city_date_top100_ordered = new_user_by_city_date_top100[order(firstdt,city)]\nnew_user_by_city_date_ordered_reshape = dcast(new_user_by_city_date_top100_ordered, city ~ firstdt, value.var = \"numbers\")\nnew_user_by_city_date_ordered_reshape$sum = rowSums(new_user_by_city_date_ordered_reshape[,c(-1,-14)],na.rm = T)\nmelt_df_city_top20 = melt(new_user_by_city_date_ordered_reshape[1:20,-ncol(new_user_by_city_date_ordered_reshape)], id.vars = \"city\", measure.vars = 2:(ncol(new_user_by_city_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\nlnew_user_by_model_date_city$l_city = str_replace(lnew_user_by_model_date_city$l_city,\"市\",\"\")\nlcity_rank = lnew_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(lsum = sum(numbers)),by = c(\"l_city\")]\nlcity_rank$lrank = frank(lcity_rank,-lsum,ties.method = \"min\")\nnew_user_by_city_date_ordered_reshape$rank = seq(new_user_by_city_date_ordered_reshape$sum)\nnew_user_by_city_date_ordered_reshape = merge(new_user_by_city_date_ordered_reshape,lcity_rank,by.x = \"city\",by.y= \"l_city\",all.x = TRUE)\nnew_user_by_city_date_ordered_reshape = new_user_by_city_date_ordered_reshape[order(new_user_by_city_date_ordered_reshape$rank),]\nrankchange = new_user_by_city_date_ordered_reshape$lrank - new_user_by_city_date_ordered_reshape$rank\nsymrankchange = ifelse(is.na(rankchange)|rankchange==0,\"\",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))\nsym = paste0(rankchange,symrankchange)\nnew_user_by_city_date_ordered_reshape$lsum = NULL\nnew_user_by_city_date_ordered_reshape$lrank = NULL\nnew_user_by_city_date_ordered_reshape = cbind(sym,new_user_by_city_date_ordered_reshape)\nrownames(new_user_by_city_date_ordered_reshape) = 1:nrow(new_user_by_city_date_ordered_reshape)\nDT::datatable(new_user_by_city_date_ordered_reshape, options = list(pageLength = 20))\n```\n\n上海，北京，南京占据前三名，**[回忆]半个月前重庆首次进入前三甲**    \n\n**重庆，北京，郑州，石家庄，武汉，天津，成都，西安，广州，深圳，青岛，厦门，大连名次上升较多**    \n**徐州，扬州，绍兴，漯河等城市下降较多**\n\n这两周，流量整体较少，暴增流量不明显\n\n```{r city_treemap}\ntreemap(melt_df_city_top20,index=c(\"date\",\"city\"),vSize = \"value\")\n```\n\n结构矩形树图，大矩形为日期，小矩形为具体城市，按从大到小的顺序，从上到下，从左到右排列\n\n##新用户不同渠道分布\n```{r channel,echo = FALSE}\nchannel_sql = paste0(\"select * from dl.dl_channel_umid_openid where substr(umid_firstonlinetime,1,10) >= '\",as.character(datestart),\"' and dt = \",dateend.str)\nnew_user_channel_unique = read_data_impala_general(channel_sql)\ncolnames(new_user_channel_unique) = str_replace(colnames(new_user_channel_unique),\"dl_channel_umid_openid.\",\"\")\nnew_user_channel_unique = data.table(new_user_channel_unique)\nnew_user_channel_unique$date = str_sub(new_user_channel_unique$umid_firstonlinetime,1,10)\nnew_user_by_channel_date = new_user_channel_unique[,.(numbers = .N),by = c(\"channel\",\"date\")]\nnew_user_by_channel = new_user_channel_unique[,.(numbers = .N),by = c(\"channel\")][order(numbers,decreasing = T)]\nchannel_list = new_user_by_channel$channel\nnew_user_by_channel_date_top = new_user_by_channel_date\nnew_user_by_channel_date_top$channels = factor(new_user_by_channel_date_top$channel,levels = channel_list)\nnew_user_by_channel_date_top_ordered = new_user_by_channel_date_top[order(date,channels)]\nnew_user_by_channel_date_ordered_reshape = dcast(new_user_by_channel_date_top_ordered, channels ~ date, value.var = \"numbers\")\nnew_user_by_channel_date_ordered_reshape$sum = rowSums(new_user_by_channel_date_ordered_reshape[,c(-1)],na.rm = T)\nDT::datatable(new_user_by_channel_date_ordered_reshape, options = list(pageLength = 20))\n#kable(channel,format = \"markdown\")\n```\n\n二维码-IOS第一,百度助手第二，iphone appstore排第三，然后是小米,360助手,华为,oppo推广,二维码-Android\n\n```{r channel_treemap}\nmelt_df_channel_top20 = melt(new_user_by_channel_date_ordered_reshape[1:20,-ncol(new_user_by_channel_date_ordered_reshape)], id.vars = \"channels\", measure.vars = 2:(ncol(new_user_by_channel_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\ntreemap(melt_df_channel_top20,index=c(\"date\",\"channels\"),vSize = \"value\")\n```\n\n结构矩形树图，大矩形为日期，小矩形为具体渠道，按从大到小的顺序，从上到下，从左到右排列\n\n#产出（通过订单转化，领券等衡量）\n##每日订单数\n```{r order}\nonline_order_sql = paste0(\"select count(distinct purchaser_id) as o_num,substr(b.create_date,1,10) as dt from\n(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o inner join ods.ods_app_pageview_info a on \n                          o.purchaser_id = a.u_id where substr(o.create_date,1,10)>= '\",datestart,\"' and o.order_status not in (1,7,19) and o.order_type=1) b\n                          group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)\")\nonline_order = read_data_impala_general(online_order_sql)\ntotal_order_sql = paste0(\"select count(distinct purchaser_id) as t_num,substr(b.create_date,1,10) as dt from\n(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o where substr(o.create_date,1,10)>= '\",datestart,\"' and o.order_status not in (1,7,19) and o.order_type=1) b\n                         group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)\")\ntotal_order = read_data_impala_general(total_order_sql)\nmatrix = as.matrix(cbind(online_num = online_order$o_num,total_num = total_order$t_num))\nmatrix = t(matrix)\npar(bg = 'white')\ncolors <- c(\"blue\",\"purple\")\ndates <- online_order$dt\nstacks <- c(\"有过APP活动订单数\",\"总订单数\")\n# Create the bar chart.\nbarplot(matrix,main=\"每日订单数\",names.arg=dates,xlab=\"month\",ylab=\"订单数\",col=colors,beside = TRUE)\n# Add the legend to the chart.\nlegend(\"topright\",stacks, cex=0.7, fill=colors,xjust = 0, yjust = 1)\n```\n\n在APP上有过活动的人生成的订单占比较少，订单次高峰发生在4月14日和4月15日，峰值有8000单左右，主高峰发生在4月21日和4月22日，峰值有10000但左右，峰值基本上是周末，平时订单很少，基本在3000单以下，周一到周日有上升趋势\n\n##家装预约\n```{r book}\nonline_book_sql = paste0(\"select substr(d.create_date,1,10) dt,count(distinct d.openid) b_num from (select b.create_date,c.openid from\n                   ods.ods_jz_business_jz_activity_user_dt b inner join \nods.ods_db_user_center_users_dt c on b.user_mobile=c.mobile\nwhere b.is_del=0 and substr(create_date,1,10)>='\",datestart,\"') d\ngroup by substr(d.create_date,1,10) order by substr(d.create_date,1,10)\")\nonline_book = read_data_impala_general(online_book_sql)\nbarplot(online_book$b_num,names.arg = online_book$dt,col = \"brown\",xlab = \"month\",ylab = \"预约数\",main = \"每日家装预约\")\n```\n\n**整体预约出现逐渐上升趋势**，峰值出现在4月20日，4月21日，4月22日三天，峰值预约高达150-250，4月16日至4月19日也有50以上    \n相比上周 *4月4日预约数最多，最高预约数100出头，平时预约数在50以下* 有了明显的改善\n\n##领券情况\n```{r coupon}\ncoupon_sql = paste0(\"select substr(d.create_time,1,10) dt,count(distinct d.open_id) c_num \nfrom (select create_time,open_id from ods.ods_marketing_center_mmc_user_coupon_dt where \nto_date(create_time)>='\",datestart,\"'\nand channel_id not in (2,4) and open_id!='') d\ngroup by substr(d.create_time,1,10) order by substr(d.create_time,1,10)\")\ncoupon = read_data_impala_general(coupon_sql)\nbarplot(coupon$c_num,names.arg = coupon$dt,xlab = \"month\",ylab = \"领券数\",main = \"每日领券情况\",col = \"blue\")\n```\n\n领券情况呈现一定周期性，每到周末都会出现峰值，且从周一到周末逐渐上升     \n这两周的趋势总体一直在上升，在4月13日，4月14日，4月15日达到小峰值，4月17日至4月22日达到大峰值，在峰值期4月19日偏低    \n\n```{r end,echo=FALSE,results=\"hide\"}\ndbDisconnect(con)\n# dbDisconnect(conn)\n```\n",
    "created" : 1514532264239.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2858555485",
    "id" : "8CC3CA2F",
    "lastKnownWriteTime" : 1524468927,
    "last_content_update" : 1524468927819,
    "path" : "~/R_Projects/uc_analysis/Rfile/User_analysis_rearrange.Rmd",
    "project_path" : "Rfile/User_analysis_rearrange.Rmd",
    "properties" : {
        "docOutlineVisible" : "0",
        "last_setup_crc32" : "2D243485fbe27d06",
        "source_window_id" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 25,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
{
    "collab_server" : "",
    "contents" : "---\ntitle: \"User_analysis\"\nauthor: \"qingye\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(readxl)\nlibrary(knitr)\nlibrary(xtable)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(plyr)\nlibrary(treemap)\nlibrary(grid)\n\nsource('~/Rfile/R_impala.R')\ndateend = Sys.Date()-1\ndatestart = Sys.Date()-2\ndatestart.str = format(datestart,'%Y%m%d')\ndateend.str = format(dateend,'%Y%m%d')\ndates = datestart.str:dateend.str\nldateend = Sys.Date()-3\nldatestart = Sys.Date()-7\nldatestart.str = format(ldatestart,'%Y%m%d')\nldateend.str = format(ldateend,'%Y%m%d')\nactivity_name = \"双十一\"\ncompare_period = \"近一周\"\noptions(scipen = 10)\n\n```\n#本报告对`r activity_name`活动，`r datestart`至`r dateend` 两天用户的行为进行了分析\n####包括反映用户数目的UV,反映用户活跃度的：人均PV,用户活跃时长,以及反映用户黏度的次日留存，本次报告加入了路径深度的分析,加入了转化部分,并采用多种方式作图\n####本次报告用R语言自动化生成报告的框架搭建，今后将在此基础上不断改进优化，下次报告将深化转化方面的分析(加入渠道分析)，加深对于路径方面的分析，引进智能化的异常值提醒和分析，考虑加入排名变动\n\n\n##近两周的日活跃度(UV)\n```{r daily_pvuv,echo = FALSE}\n#1daily activity pvuv\npvuv_sql = paste0(\"select dt,\n                 count(1) uv,\n                 sum(pv) pv ,\n                 sum(pv)/count(1) perpv,\n                 count(if(isnew='new',1,null)) newuv ,\n                 sum(if(isnew='new',pv,0)) newpv, \n                 sum(if(isnew='new',pv,0))/count(if(isnew='new',1,null)) newperpv,\n                 count(if(isnew='old',1,null)) olduv ,\n                 sum(if(isnew='old',pv,0)) oldpv, \n                 sum(if(isnew='old',pv,0))/count(if(isnew='old',1,null)) oldperpv\n                 from \n                 dl.umid_pv where dt>='\",ldatestart.str,\n                 \"'group by dt\")\nmpvuv = read_data_impala_general(pvuv_sql)\nmpvuv = mpvuv[order(mpvuv$dt),]\nxlarge = nrow(mpvuv)\nxtopic = xlarge - 2\npvuv = mpvuv[xtopic:xlarge,]\nwpvuv = pvuv\npvuv$fdt = as.factor(pvuv$dt)\nmatrix = as.matrix(cbind(newuv = pvuv$newuv,olduv = pvuv$olduv))\nmatrix = t(matrix)\nmaxy = max(pvuv$uv)\nmaxx = pvuv[which.max(pvuv$uv),\"dt\"]\nmeany = mean(pvuv$uv)\nnewbetterthanold = sum(pvuv$newuv>pvuv$olduv)\nabnormalhighdt = pvuv[pvuv$uv>1.3*meany,\"dt\"]\nabnormallowdt = pvuv[pvuv$uv<0.7*meany,\"dt\"]\npar(bg = 'white')\ncolors <- c(\"green\",\"red\")\ndates <- pvuv$dt\nstacks <- c(\"New\",\"Old\")\n# Create the bar chart.\nbarplot(matrix,main=\"Every day uv\",names.arg=dates,xlab=\"date\",ylab=\"uv\",col=colors)\n# Add the legend to the chart.\nlegend(x = 0,y = maxy, stacks, cex=0.7, fill=colors,xjust = 0, yjust = 1)\n```\n\n平均UV为`r round(meany)`,\n\n最大UV为`r round(maxy)`,\n\n最大值发生在`r maxx`\n\n有`r newbetterthanold` 天新用户在UV值上超过老用户\n\n`r ifelse(length(abnormalhighdt)==0,\"并没有哪天\",abnormalhighdt)` 用户UV明显高于平均值\n`r ifelse(length(abnormallowdt)==0,\"并没有哪天\",abnormallowdt)` 用户UV明显低于平均值\n\n\n##`r activity_name`人均PV\n```{r pv_perperson,echo = FALSE}\n# par(bg = 'black')\ncolors = rainbow(3)\nylarge = max(max(pvuv$perpv),max(pvuv$newperpv),max(pvuv$oldperpv))\ntmax = max(pvuv$perpv)\ndttmax = pvuv[which.max(pvuv$perpv),\"dt\"]\ntmin = min(pvuv$perpv)\ndttmin = pvuv[which.min(pvuv$perpv),\"dt\"]\ntmean = mean(pvuv$perpv)\n\nnmax = max(pvuv$newperpv)\ndtnmax = pvuv[which.max(pvuv$newperpv),\"dt\"]\nnmin = min(pvuv$newperpv)\ndtnmin = pvuv[which.min(pvuv$newperpv),\"dt\"]\nnmean = mean(pvuv$newperpv)\n\nomax = max(pvuv$oldperpv)\ndtomax = pvuv[which.max(pvuv$oldperpv),\"dt\"]\nomin = min(pvuv$oldperpv)\ndtomin = pvuv[which.min(pvuv$oldperpv),\"dt\"]\nomean = mean(pvuv$oldperpv)\n\nnewbetterthanold = sum(pvuv$newperpv>pvuv$oldperpv)\ndifftrend = pvuv[!(c(1,sign(diff(pvuv$newperpv)*diff(pvuv$oldperpv)))+1),\"dt\"]\nmatrix = as.matrix(cbind(newuv = pvuv$newperpv,olduv = pvuv$oldperpv))\nmatrix = t(matrix)\n#考虑人均pv\npar(bg = 'white')\ncolors <- c(\"green\",\"red\")\ndates <- pvuv$dt\nstacks <- c(\"New\",\"Old\")\n# Create the bar chart.\nbarplot(matrix,main=\"每日人均PV\",names.arg=dates,xlab=\"date\",ylab=\"人均pv\",col=colors,beside = TRUE)\nlegend('topright',c(\"newperpv\",\"oldperpv\"),cex = 0.7,fill = colors,text.col= 'black')\n```\n\n总体人均PV均值为`r tmean` \n\n总体人均PV最大值为`r tmax` 最大值发生在 `r dttmax`\n\n总体人均PV最小值为`r tmin` 最小值发生在 `r dttmin`\n\n新用户人均PV均值为`r nmean` \n\n新用户人均PV最大值为`r nmax` 最大值发生在 `r dtnmax`\n\n新用户人均PV最小值为`r nmin` 最小值发生在 `r dtnmin`\n\n老用户人均PV均值为`r omean` \n\n老用户人均PV最大值为`r omax` 最大值发生在 `r dtomax`\n\n老用户人均PV最小值为`r omin` 最小值发生在 `r dtomin`\n\n有`r newbetterthanold` 天新用户在人均PV值上超过老用户\n\n`r difftrend` 新用户和老用户的趋势不一样\n\n##PV 趋势图，篮框内为聚焦时期\n```{r pvuv_trend, echo=FALSE}\npm <- ggplot(wpvuv, aes(as.Date(dt,format = \"%Y%m%d\"), pv)) + labs(x = \"DATE\",y = \"PV\") + ylim(0,max(wpvuv$pv,na.rm = T))\nmainplot <- pm + geom_line(colour = I(\"purple\")) + labs(title = paste0(activity_name,\"PV趋势变化\"))\np = ggplot(mpvuv,aes(as.Date(dt,format = \"%Y%m%d\"), pv))+labs(x = \"DATE\",y = \"PV\") + ylim(min(mpvuv$pv,na.rm = T),max(mpvuv$pv,na.rm = T))\np1 <- p + geom_rect(aes(xmin = as.Date(mpvuv$dt[xtopic],format = \"%Y%m%d\"), xmax = as.Date(mpvuv$dt[xlarge],format = \"%Y%m%d\"),\n                        ymin = min(mpvuv$pv, na.rm = TRUE), ymax = max(mpvuv$pv, na.rm = TRUE)),fill = alpha(\"lightblue\", 0.2))\nsubplot <- p1 + geom_line(colour = I(\"grey\"),size = 0.8) \nvp <- viewport(width = 0.4, height = 0.4, x = 1,\n               y = unit(0.7, \"lines\"), just = c(\"right\",\"bottom\"))\nfull <- function() {\n  print(mainplot)\n  print(subplot, vp = vp)\n}\nfull()\n```\n\n画中画分析，大图，小图高亮的那部分，表示示`r activity_name`pv趋势，而小图表示`r compare_period`的pv情况\n`r activity_name`比过去`r compare_period`整体pv要高\n\n## 用户活跃时长\n```{r time_span, echo=FALSE}\ntime_span_sql = paste0(\"select dt,\n  avg(persvg) totalavg,\n  avg(case when isnew ='new' then persvg else null end) newavg,\n  avg(case when isnew ='old' then persvg else null end) oldavg\nfrom\n(\n  select a.dt,a.u_mid,\n  sum(CAST(p_stay_time AS INT))/1000/60 persvg,\n  case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end isnew \n  from \n  ods.ods_app_pageview_info a \n  left outer join \n  dl.umid_firstonlinetime b on a.u_mid=b.u_mid \n  where a.dt>=\",ldateend.str,\" and b.dt='\",dateend.str,\"' and\n  p_domain='mmall.com' and service like '%staytime%' and substr(a.u_mid,1,2)!='a_' and path='z'  and l_city!='测试'\n  and p_type not in ('page.closedown','page.wakeup','page.activate.main') and length(p_stay_time)<=7\n  group by a.dt,a.u_mid,case when regexp_replace(to_date(firstonlinetime),'-','')=CAST(a.dt AS STRING) then 'new' else 'old' end \n)a group by dt\")\n  time_span = read_data_impala_general(time_span_sql)\n  time_span = time_span[order(time_span$dt),]\n  # write.xlsx(time_span,\"~/data/uc_analysis/time_span.xlsx\")\n  time_span$fdt = as.factor(time_span$dt)\n  time_span.new = time_span[,-1]\n  timespan.m <- melt(time_span.new)\n  timespan.m <- ddply(timespan.m, .(variable), transform,rescale = rescale(value))\n  timespan.m$dates = str_sub(as.character(timespan.m$fdt),5,8)\n  names(timespan.m) = c(\"fdt\",\"old_new_user\",\"time_span\",\"rescale\",\"dates\")\n  (p <- ggplot(timespan.m, aes(dates,old_new_user)) + geom_tile(aes(fill = time_span),colour = \"white\") + scale_fill_gradient(low = \"white\",high = \"purple\"))\n```\n\n对于用户访问时长作的热力图，可以看出新老用户差距明显，新用户大部分日子访问时长接近白色\n双十一当天的用户访问时长明显高过11月10日以及11月12日，无论新用户还是老用户\n\n## 次日留存\n```{r survival,echo=FALSE}\n  survival_sql = paste0(\"select \n  a.dt,ndv(a.u_mid) t,\n  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 then a.u_mid else null end)\tt1,\n  ndv(if(a.isnew='new',a.u_mid,null)) newuv ,\n  ndv(case when datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1 and a.isnew='new' then a.u_mid else null end) newt1,\n  ndv(if(a.isnew='old',a.u_mid,null)) olduv,\n  ndv(case when  datediff(concat(substr(b.dt,1,4),'-',substr(b.dt,5,2),'-',substr(b.dt,7,2)),\n  concat(substr(a.dt,1,4),'-',substr(a.dt,5,2),'-',substr(a.dt,7,2)))=1  and a.isnew='old' then a.u_mid else null end) noldt1\n  from dl.umid_pv a\n  left outer join\n  dl.umid_pv b on a.u_mid=b.u_mid\t\t\n  where  a.dt>='\",ldateend.str,\"' group by a.dt\")\n\nsurvival = read_data_impala_general(survival_sql)\nsurvival = survival[order(survival$dt),]\nsurvival = survival[-nrow(survival),]\nsurvival$fdt = as.factor(survival$dt)\npar(bg = 'black')\ncolors = rainbow(3)\n#考虑survival,!!须注意最后一天\nylarge = max(max(survival$t1/survival$t),max(survival$newt1/survival$newuv),max(survival$noldt1/survival$olduv))\nmatrix = as.matrix(cbind(newsurvival = survival$newt1/survival$newuv,oldsurvival = survival$noldt1/survival$olduv, totalsurvival = survival$t1/survival$t))\nmatrix = t(matrix)\n#考虑人均pv\npar(bg = 'white')\ncolors <- rainbow(3)\ndates <- survival$dt\nstacks <- colors\n# Create the bar chart.\nbarplot(matrix,main=paste0(activity_name,\"留存率\"),names.arg=dates,xlab=\"date\",ylab=\"留存率\",col=colors,beside = TRUE)\nlegend('topright',c(\"新用户留存\",\"老用户留存\",\"总体留存\"),cex = 0.7,fill = colors,text.col= 'black')\n\n```\n\n次日留存率，新用户并没有明显的波动，几乎保持水平，整体的波动几乎全部来源于老用户，老用户波动也很小，11月8日留存率有个小小的下降趋势\n\n##新老用户访问平均深度（按人头）\n```{r depth_person_date}\ndepth_p_sql = paste0(\"select t.isnew,t.dt,sum(cast(u_depth as INT))/count(t.u_mid) avg_depth, count(t.u_mid) p_num from \n(select a.dt,isnew,max(b.depth) as u_depth,a.u_mid from \ndm.dm_app_umid_step a  \nleft outer join \ntest.pagelevel b on a.page_name_zh=b.page_name where length(b.depth)=1 and dt >=  '\",ldateend.str,\"' group by a.dt,a.isnew,a.u_mid) t group by t.isnew,t.dt\")\ndepth_p = read_data_impala_general(depth_p_sql)\ndepth_p = depth_p[order(depth_p$dt),]\np  = ggplot(depth_p,aes(dt,avg_depth))\np + geom_bar(aes(fill = isnew),stat = \"identity\",position = \"dodge\") + labs(x = \"日期\",y = \"平均深度\",title = \"平均访问深度（按用户）\")\n```\n\n按人头得出的访问平均深度上，新用户接近超过2，老用户超过2.5\n双十一当天用户访问深度最高\n\n##新用户不同机型分布\n```{r model,echo = FALSE}\nnew_user_by_model_date_city_sql = \npaste0(\"select count(b.u_mid) numbers,firstdt,d_model,l_city from\n(select f.u_mid,substr(firstonlinetime,1,10) firstdt,a.d_model,a.l_city from dl.umid_firstonlinetime f left join\n  (select * from \n  (select u_mid,d_model,l_city,ROW_NUMBER() OVER (PARTITION BY u_mid,d_model,l_city ORDER BY dt) \n       as level from ods.ods_app_pageview_info where dt>=\",ldatestart.str,\") t where t.level = 1) a\n  using(u_mid) where substr(firstonlinetime,1,10) >= '\",as.character(ldatestart),\"' \n  and f.dt = '\",dateend.str,\"') b group by firstdt,d_model,l_city\")\nnew_user_by_model_date_city = read_data_impala_general(new_user_by_model_date_city_sql)\nnew_user_by_model_date_city = data.table(new_user_by_model_date_city)\nlnew_user_by_model_date_city = new_user_by_model_date_city[firstdt < as.character(datestart),]\nnew_user_by_model_date_city = new_user_by_model_date_city[firstdt >= as.character(datestart),]\nnew_user_by_model_date = new_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(numbers = sum(numbers)),by = c(\"d_model\",\"firstdt\")]\nnew_user_by_model = new_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(numbers = sum(numbers)),by = \"d_model\"][order(numbers,decreasing = T)]\nmodel_list = new_user_by_model[1:100,]$d_model\nnew_user_by_model_date_top100 = new_user_by_model_date[d_model %in% model_list,]\nnew_user_by_model_date_top100$model = factor(new_user_by_model_date_top100$d_model,levels = model_list)\nnew_user_by_model_date_top100_ordered = new_user_by_model_date_top100[order(firstdt,model)]\nnew_user_by_model_date_ordered_reshape = reshape(new_user_by_model_date_top100_ordered, idvar = \"model\", timevar = \"firstdt\", direction = \"wide\")\nnew_user_by_model_date_ordered_reshape = dcast(new_user_by_model_date_top100_ordered, model ~ firstdt, value.var = \"numbers\")\nnew_user_by_model_date_ordered_reshape$sum = rowSums(new_user_by_model_date_ordered_reshape[,-1],na.rm = T)\nmelt_df_model_top20 = melt(new_user_by_model_date_ordered_reshape[1:20,-ncol(new_user_by_model_date_ordered_reshape)], id.vars = \"model\", measure.vars = 2:(ncol(new_user_by_model_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\nlmodel_rank = lnew_user_by_model_date_city[d_model!=\"null\" & str_trim(d_model)!=\"\",.(lsum = sum(numbers)),by = c(\"d_model\")]\nlmodel_rank$lrank = frank(lmodel_rank,-lsum,ties.method = \"min\")\nnew_user_by_model_date_ordered_reshape$rank = seq(new_user_by_model_date_ordered_reshape$sum)\nnew_user_by_model_date_ordered_reshape = merge(new_user_by_model_date_ordered_reshape,lmodel_rank,by.x = \"model\",by.y= \"d_model\",all.x = TRUE)\nnew_user_by_model_date_ordered_reshape = new_user_by_model_date_ordered_reshape[order(new_user_by_model_date_ordered_reshape$rank),]\nrankchange = new_user_by_model_date_ordered_reshape$lrank - new_user_by_model_date_ordered_reshape$rank\nsymrankchange = ifelse(is.na(rankchange)|rankchange==0,\"\",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))\nsym = paste0(rankchange,symrankchange)\nnew_user_by_model_date_ordered_reshape$lsum = NULL\nnew_user_by_model_date_ordered_reshape$lrank = NULL\nnew_user_by_model_date_ordered_reshape = cbind(sym,new_user_by_model_date_ordered_reshape)\nrow.names(new_user_by_model_date_ordered_reshape) = new_user_by_model_date_ordered_reshape$rank\nDT::datatable(new_user_by_model_date_ordered_reshape, options = list(pageLength = 20))\n```\n\n##新用户不同城市分布\n```{r city,echo = FALSE}\nnew_user_by_model_date_city$l_city = str_replace(new_user_by_model_date_city$l_city,\"市\",\"\")\nnew_user_by_city_dt = new_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(numbers = sum(numbers)),by = c(\"l_city\",\"firstdt\")]\nnew_user_by_city = new_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(numbers = sum(numbers)),by = \"l_city\"][order(numbers,decreasing = T),]\ncity_list = new_user_by_city[1:101,]$l_city\nnew_user_by_city_date_top100 = new_user_by_city_dt[l_city %in% city_list,]\nnew_user_by_city_date_top100$city = factor(new_user_by_city_date_top100$l_city,levels = city_list)\nnew_user_by_city_date_top100_ordered = new_user_by_city_date_top100[order(firstdt,city)]\nnew_user_by_city_date_ordered_reshape = dcast(new_user_by_city_date_top100_ordered, city ~ firstdt, value.var = \"numbers\")\nnew_user_by_city_date_ordered_reshape$sum = rowSums(new_user_by_city_date_ordered_reshape[,c(-1,-14)],na.rm = T)\nmelt_df_city_top20 = melt(new_user_by_city_date_ordered_reshape[1:20,-ncol(new_user_by_city_date_ordered_reshape)], id.vars = \"city\", measure.vars = 2:(ncol(new_user_by_city_date_ordered_reshape)-1), variable.name = \"date\", value.name = \"value\")\nlnew_user_by_model_date_city$l_city = str_replace(lnew_user_by_model_date_city$l_city,\"市\",\"\")\nlcity_rank = lnew_user_by_model_date_city[!l_city %in% c(\"null\",\"局域网\",\"未知\"),.(lsum = sum(numbers)),by = c(\"l_city\")]\nlcity_rank$lrank = frank(lcity_rank,-lsum,ties.method = \"min\")\nnew_user_by_city_date_ordered_reshape$rank = seq(new_user_by_city_date_ordered_reshape$sum)\nnew_user_by_city_date_ordered_reshape = merge(new_user_by_city_date_ordered_reshape,lcity_rank,by.x = \"city\",by.y= \"l_city\",all.x = TRUE)\nnew_user_by_city_date_ordered_reshape = new_user_by_city_date_ordered_reshape[order(new_user_by_city_date_ordered_reshape$rank),]\nrankchange = new_user_by_city_date_ordered_reshape$lrank - new_user_by_city_date_ordered_reshape$rank\nsymrankchange = ifelse(is.na(rankchange)|rankchange==0,\"\",ifelse(rankchange>0,intToUtf8(9650),intToUtf8(9660)))\nsym = paste0(rankchange,symrankchange)\nnew_user_by_city_date_ordered_reshape$lsum = NULL\nnew_user_by_city_date_ordered_reshape$lrank = NULL\nnew_user_by_city_date_ordered_reshape = cbind(sym,new_user_by_city_date_ordered_reshape)\nDT::datatable(new_user_by_city_date_ordered_reshape, options = list(pageLength = 20))\n```\n\n##新用户不同渠道分布\n```{r channel,echo = FALSE}\nchannel_sql = paste0(\"select * from dl.dl_channel_umid_openid where substr(umid_firstonlinetime,1,10) >= '\",as.character(datestart),\"' and dt = \",dateend.str)\nnew_user_channel_unique = read_data_impala_general(channel_sql)\ncolnames(new_user_channel_unique) = str_replace(colnames(new_user_channel_unique),\"dl_channel_umid_openid.\",\"\")\nnew_user_channel_unique = data.table(new_user_channel_unique)\nnew_user_channel_unique$date = str_sub(new_user_channel_unique$umid_firstonlinetime,1,10)\nnew_user_by_channel_date = new_user_channel_unique[,.(numbers = .N),by = c(\"channel\",\"date\")]\nnew_user_by_channel = new_user_channel_unique[,.(numbers = .N),by = c(\"channel\")][order(numbers,decreasing = T)]\nchannel_list = new_user_by_channel$channel\nnew_user_by_channel_date_top = new_user_by_channel_date\nnew_user_by_channel_date_top$channels = factor(new_user_by_channel_date_top$channel,levels = channel_list)\nnew_user_by_channel_date_top_ordered = new_user_by_channel_date_top[order(date,channels)]\nnew_user_by_channel_date_ordered_reshape = dcast(new_user_by_channel_date_top_ordered, channels ~ date, value.var = \"numbers\")\nnew_user_by_channel_date_ordered_reshape$sum = rowSums(new_user_by_channel_date_ordered_reshape[,c(-1)],na.rm = T)\nDT::datatable(new_user_by_channel_date_ordered_reshape, options = list(pageLength = 20))\n#kable(channel,format = \"markdown\")\n```\n\n\n#转化问题\n##每日订单数\n```{r order}\nonline_order_sql = paste0(\"select count(distinct purchaser_id) as o_num,substr(b.create_date,1,10) as dt from\n(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o inner join ods.ods_app_pageview_info a on \n                          o.purchaser_id = a.u_id where substr(o.create_date,1,10)>= '\",ldateend,\"' and o.order_status not in (1,7,19) and o.order_type=1) b\n                          group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)\")\nonline_order = read_data_impala_general(online_order_sql)\ntotal_order_sql = paste0(\"select count(distinct purchaser_id) as t_num,substr(b.create_date,1,10) as dt from\n(select purchaser_id,create_date from ods.ods_tx_order_tx_order_dt o where substr(o.create_date,1,10)>= '\",ldateend,\"' and o.order_status not in (1,7,19) and o.order_type=1) b\n                         group by substr(b.create_date,1,10) order by substr(b.create_date,1,10)\")\ntotal_order = read_data_impala_general(total_order_sql)\nmatrix = as.matrix(cbind(online_num = online_order$o_num,total_num = total_order$t_num))\nmatrix = t(matrix)\npar(bg = 'white')\ncolors <- c(\"blue\",\"purple\")\ndates <- online_order$dt\nstacks <- c(\"有过APP活动订单数\",\"总订单数\")\n# Create the bar chart.\nbarplot(matrix,main=\"每日订单数\",names.arg=dates,xlab=\"month\",ylab=\"订单数\",col=colors,beside = TRUE)\n# Add the legend to the chart.\nlegend(\"topright\",stacks, cex=0.7, fill=colors,xjust = 0, yjust = 1)\n```\n\n##家装预约\n```{r book}\nonline_book_sql = paste0(\"select substr(d.create_date,1,10) dt,count(distinct d.openid) b_num from (select b.create_date,c.openid from\n                   ods.ods_jz_business_jz_activity_user_dt b inner join \nods.ods_db_user_center_users_dt c on b.user_mobile=c.mobile\nwhere b.is_del=0 and substr(create_date,1,10)>='\",ldateend,\"') d\ngroup by substr(d.create_date,1,10) order by substr(d.create_date,1,10)\")\nonline_book = read_data_impala_general(online_book_sql)\nbarplot(online_book$b_num,names.arg = online_book$dt,col = \"brown\",xlab = \"month\",ylab = \"预约数\",main = \"每日家装预约\")\n```\n\n##领券情况\n```{r coupon}\ncoupon_sql = paste0(\"select substr(d.create_time,1,10) dt,count(distinct d.open_id) c_num \nfrom (select create_time,open_id from ods.ods_marketing_center_mmc_user_coupon_dt where \nto_date(create_time)>='\",ldateend,\"'\nand channel_id not in (2,4) and open_id!='') d\ngroup by substr(d.create_time,1,10) order by substr(d.create_time,1,10)\")\ncoupon = read_data_impala_general(coupon_sql)\ncoupon = coupon[-nrow(coupon),]\nbarplot(coupon$c_num,names.arg = coupon$dt,xlab = \"month\",ylab = \"领券数\",main = \"每日领券情况\",col = \"blue\")\n```\n\n",
    "created" : 1510541406145.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "790705680",
    "id" : "85EC6D16",
    "lastKnownWriteTime" : 1510567681,
    "last_content_update" : 1510567681334,
    "path" : "~/R_Projects/uc_analysis/Rfile/User_analysis_topic.rmd",
    "project_path" : "Rfile/User_analysis_topic.rmd",
    "properties" : {
        "last_setup_crc32" : "",
        "source_window_id" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 17,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
require(gbm)
require(MASS)#package with the boston housing dataset
#separating training and test data
train=sample(1:506,size=374)
View(Boston)
Boston.boost=gbm(medv ~ . ,data = Boston[train,],distribution = "gaussian",n.trees = 10000,
shrinkage = 0.01, interaction.depth = 4)
require(gbm)
install.packages("gbm")
require(gbm)
require(MASS)#package with the boston housing dataset
Boston.boost=gbm(medv ~ . ,data = Boston[train,],distribution = "gaussian",n.trees = 10000,
shrinkage = 0.01, interaction.depth = 4)
View(Boston)
summary(Boston.boost) #Summary gives a table of Variable Importance and a plot of Variable Importance
?Boston
Boston.boost
plot(Boston.boost,i="lstat")
plot(Boston.boost,i="rm")
cor(Boston$lstat,Boston$medv)#negetive correlation coeff-r
cor(Boston$rm,Boston$medv)#positive correlation coeff-r
n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values
predmatrix<-predict(Boston.boost,Boston[-train,],n.trees = n.trees)
dim(predmatrix) #dimentions of the Prediction Matrix
dim(Boston.boost)
length(Boston.boost)
medv
test.error<-with(Boston[-train,],apply( (predmatrix-medv)^2,2,mean))
head(test.error) #contains the Mean squared test error for each of the 100 trees averaged
plot(n.trees , test.error , pch=19,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")
abline(h = min(test.err),col="red") #test.err is the test error of a Random forest fitted on same data
abline(h = min(test.error),col="red") #test.err is the test error of a Random forest fitted on same data
library(adabag);
install.packages("adabag")
adadata<-read.csv('~/data/bank-full.csv',
header=TRUE,sep=";")
View(adadata)
adaboost<-boosting(y~age+job+marital+education+default+balance+
housing+loan+contact+day+month+duration+campaign+pdays+previous+
poutcome, data=adadata, boos=TRUE, mfinal=20,coeflearn='Breiman')
library(adabag)
summary(adaboost)
adaboost<-boosting(y~age+job+marital+education+default+balance+
housing+loan+contact+day+month+duration+campaign+pdays+previous+
poutcome, data=adadata, boos=TRUE, mfinal=20,coeflearn='Breiman')
summary(adaboost)
adaboost$trees
adaboost$importance
errorevol(adaboost,adadata)
predict(adaboost,adadata)
t1<-adaboost$trees[[1]]
library(tree)
?adabag::predict.boosting
temp = predict(adaboost,adadata)
plot(n.trees , test.error , pch=19,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")
View(predmatrix)
View(Boston)
require(gbm)
require(MASS)#package with the boston housing dataset
train=sample(1:506,size=374)
View(Boston)
Boston.boost=gbm(medv ~ . ,data = Boston[train,],distribution = "gaussian",n.trees = 10000,
shrinkage = 0.01, interaction.depth = 4)
plot(Boston.boost,i="lstat")
plot(Boston.boost,i="rm")
cor(Boston$lstat,Boston$medv)#negetive correlation coeff-r
cor(Boston$rm,Boston$medv)#positive correlation coeff-r
n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values
n.trees
Boston.boost
summary(Boston.boost) #Summary gives a table of Variable Importance and a plot of Variable Importance
predmatrix<-predict(Boston.boost,Boston[-train,],n.trees = n.trees)
dim(predmatrix) #dimentions of the Prediction Matrix
View(predmatrix)
test.error<-with(Boston[-train,],apply( (predmatrix-medv)^2,2,mean))
View(test.error)
test.error
plot(n.trees , test.error , pch=19,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")
length(Boston)
library(randomForest)
rf = randomForest(medv ~ . ,data = Boston[train,])
rf.pred = predict(rf,Boston[-train,-14])
e.rf = crossprod(rf.pred - unlist(Boston[-train,14]))/nrow(Boston[-train,])
abline(h = e.rf,col="red") #test.err is the test error of a Random forest fitted on same data
Boston.boost
summary(Boston.boost) #Summary gives a table of Variable Importance and a plot of Variable Importance
library(mlbench)
library(caret)
# load data
data(PimaIndiansDiabetes)
# rename dataset to keep code below generic
dataset <- PimaIndiansDiabetes
View(dataset)
?trainControl
?resamples
results <- resamples(list(lda=fit.lda, logistic=fit.glm, glmnet=fit.glmnet,
svm=fit.svmRadial, knn=fit.knn, nb=fit.nb, cart=fit.cart, c50=fit.c50,
bagging=fit.treebag, rf=fit.rf, gbm=fit.gbm))
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
preProcess=c("center", "scale")
set.seed(seed)
fit.lda <- train(diabetes~., data=dataset, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
set.seed(seed)
fit.glm <- train(diabetes~., data=dataset, method="glm", metric=metric, trControl=control)
set.seed(seed)
fit.glmnet <- train(diabetes~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
set.seed(seed)
fit.glmnet <- train(diabetes~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
set.seed(seed)
fit.svmRadial <- train(diabetes~., data=dataset, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
set.seed(seed)
fit.knn <- train(diabetes~., data=dataset, method="knn", metric=metric, preProc=c("center", "scale"), trControl=control)
set.seed(seed)
fit.nb <- train(diabetes~., data=dataset, method="nb", metric=metric, trControl=control)
set.seed(seed)
fit.cart <- train(diabetes~., data=dataset, method="rpart", metric=metric, trControl=control)
set.seed(seed)
fit.c50 <- train(diabetes~., data=dataset, method="C5.0", metric=metric, trControl=control)
set.seed(seed)
fit.treebag <- train(diabetes~., data=dataset, method="treebag", metric=metric, trControl=control)
set.seed(seed)
fit.treebag <- train(diabetes~., data=dataset, method="treebag", metric=metric, trControl=control)
set.seed(seed)
fit.rf <- train(diabetes~., data=dataset, method="rf", metric=metric, trControl=control)
set.seed(seed)
fit.gbm <- train(diabetes~., data=dataset, method="gbm", metric=metric, trControl=control, verbose=FALSE)
results <- resamples(list(lda=fit.lda, logistic=fit.glm, glmnet=fit.glmnet,
svm=fit.svmRadial, knn=fit.knn, nb=fit.nb, cart=fit.cart, c50=fit.c50,
bagging=fit.treebag, rf=fit.rf, gbm=fit.gbm))
summary(results)
bwplot(results)
fit.svmRadial
fit.cart
results
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class,
## the outcome data are needed
p = .75,
## The percentage of data in the
## training set
list = FALSE)
training <- Sonar[ inTrain,]
testing <- Sonar[-inTrain,]
View(training)
plsFit <- train(Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"))
set.seed(123)
ctrl <- trainControl(method = "repeatedcv",
repeats = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary)
plsFit <- train(Class ~ .,
data = training,
method = "pls",
tuneLength = 15,
trControl = ctrl,
metric = "ROC",
preProc = c("center", "scale"))
plsFit
plot(plsFit)
f = function(n){
if(n>1){
return(n*f(n-1))
}
if(n==1){
return(1)
}
}
f(10)
f(3)
f(4)
f(5)
f(6)
f()
f = function(n=100){
if(n>1){
return(n*f(n-1))
}
if(n==1){
return(1)
}
}
f()
f = function(n=6){
if(n>1){
return(n*f(n-1))
}
if(n==1){
return(1)
}
}
f()
require(randomForest)
require(MASS)#Package which contains the Boston housing dataset
attach(Boston)
set.seed(101)
dim(Boston)

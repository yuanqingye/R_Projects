{
    "collab_server" : "",
    "contents" : "source('~/Rfile/R_impala.R')\nlibrary(caret)\nlibrary(ggplot2)\nluohe_set = unique(base[base$l_city %in% c(\"漯河\",\"漯河市\",\"luohe\",\"luoheshi\"),]$l_ip)\nView(base[!(base$l_ip %in% luohe_set),])\nsource('~/R_Projects/abnormal_activity_analysis/Rfile/test_func.R', encoding = 'UTF-8')\n#\ncity = ''\ndate.str = '20171105'\ndate.dash = '2017-11-05'\n\npage_perc_limit = paste0(\"select count(*) p_count,p_type from dm.dm_app_pageview_info where dt = '\",\n                         date.str,\"' and l_city like '\",city,\"%' and path = 'z' group by p_type\")\npage_perc_limit2 = paste0(\"select count(*) all_count from dm.dm_app_pageview_info where dt = '\",\n                         date.str,\"' and l_city like '\",city,\"%' and path = 'z'\")\npage_perc_sql = paste0(\"select t.p_count,ac.all_count,t.p_count/ac.all_count rate from\n(\",page_perc_limit,\") t cross join (\",page_perc_limit2,\") ac order by t.p_count desc\")\n\nsytemp = read_data_impala_general(page_perc_sql)\n\n#??\n#'10.11.29.197','10.11.29.146','10.11.30.9','10.11.29.155'\n\npage_perc_limit = \"select count(*) p_count,p_type from dm.dm_app_pageview_info where l_ip in \n('10.11.29.197','10.11.29.146','10.11.30.9','10.11.29.155') and path = 'z' and dt > '20171220'\"\npage_perc_limit2 = \"select count(*) all_count from dm.dm_app_pageview_info where l_ip in \n('10.11.29.197','10.11.29.146','10.11.30.9','10.11.29.155') and path = 'z' and dt > '20171220'\"\npage_perc_sql = paste0(\"select t.p_count,ac.all_count,t.p_count/ac.all_count rate from\n(\",page_perc_limit,\"group by p_type) t cross join (\",page_perc_limit2,\") ac order by t.p_count desc\")\n\nztemp = read_data_impala_general(page_perc_sql)\n\n#orginal sql\npage_perc_sql = \"select t.p_count,ac.all_count,t.p_count/ac.all_count rate from\n(select count(*) p_count,p_type from dm.dm_app_pageview_info where dt = '20171106' and l_city like '娌堥槼%' \nand path = 'z' group by p_type) t cross join (select count(*) all_count from dm.dm_app_pageview_info where dt = '20171106' and l_city like '娌堥槼%' \nand path = 'z') ac order by t.p_count desc\"\n\ncity ='閲嶅簡'\nspec.date.str = '20171209'\npv_umid_unit_min_sql = paste0(\"select u_mid,pcount,stay_time,pcount/stay_time pv from (\nselect u_mid,case when count(*)>1 then (max(cast(ts as bigint))-min(cast(ts as bigint)))/1000/60 \n                else cast(max(p_stay_time) as integer)/1000/60 end stay_time,\n                count(*) pcount from dm.dm_app_pageview_info where l_city like '\",city,\"%' and dt = '\",spec.date.str,\"' \n                and path = 'z' group by u_mid,session) t\")\npv_umid_unit_min = read_data_impala_general(pv_umid_unit_min_sql)\n\ncity = ''\nspec.date.str = '2017-12-20'\norder_umid_sql = paste0(\"select distinct u_mid from ods.ods_tx_order_tx_order_dt o \ninner join ods.ods_app_pageview_info a on \no.purchaser_id = a.u_id where substr(o.create_date,1,10)= '\",spec.date.str,\"' \nand o.order_status not in (1,7,19) and o.order_type=1 and a.l_city like '\",city,\"%'\")\n\nlast_long_umid_sql = \"select distinct u_mid from \n(select u_mid,(max(unix_timestamp(`system_time`, 'yyyy-MM-dd HH:mm:ss'))-min(unix_timestamp(`system_time`, 'yyyy-MM-dd HH:mm:ss')))/(3600*24) time_inv from dm.dm_app_pageview_info group by u_mid) t where time_inv>60\"\n\norder_umid = read_data_impala_general(order_umid_sql)\n\nlibrary(mclust)\nBIC <- mclustBIC(test_set[complete.cases(test_set),!(names(test_set)%in%c(\"sign\"))])\nplot(BIC)\nsummary(BIC)\n\ntemp_mEst = mstep(modelName = \"VII\", data = test_set[,!(names(test_set)%in%c(\"sign\"))],z = unmap(test_set[,\"sign\"]))\ntest_em_user = em(modelName = temp_mEst$modelName,data = test_set[,!(names(test_set)%in%c(\"sign\"))],\n                  parameters = temp_mEst$parameters)\nem_test_result = ifelse(test_em_user$z[,1]-test_em_user$z[,2]>0,0,1)\nem_test_list = calf1(y,em_test_result)\nem_test_AUC = calAUC(y,em_test_result)\n\nset.seed(1000)\nrand_z = runif(nrow(train_sample),0,1)\nrand_z = cbind(rand_z,(1-rand_z))\nrand_z = ifelse(rand_z>0.5,1,0)\n#\"VII\",\"VEV\"\n#VOLUME SHAPE ORIENTATION\nmEst = mstep(modelName = \"EEI\", data = train_set[,!(names(train_set)%in%c(\"sign\"))],z = unmap(train_set[,\"sign\"]))\nresult_em_user = em(modelName = mEst$modelName,data = test_set[,!(names(test_set)%in%c(\"sign\"))],\n                    parameters = mEst$parameters)\nem_result = ifelse(result_em_user$z[,1]-result_em_user$z[,2]>0,0,1)\nem_list = calf1(test_set[,\"sign\"],em_result)\nem_AUC = calAUC(test_set[,\"sign\"],em_result)\n\n#part has the same result as whole above\n#but with test the recall rate is down which is wanted since spec and precise both up\nmtEst = mstep(modelName = \"VEV\", data = train_sample[,!(names(train_sample)%in%c(\"sign\"))],z = unmap(train_sample[,\"sign\"]))\nresult_emt_user = em(modelName = mtEst$modelName,data = test_sample[,!(names(train_set)%in%c(\"sign\"))],\n                     parameters = mtEst$parameters)\nemt_result = ifelse(result_emt_user$z[,1]-result_emt_user$z[,2]>0,0,1)\nemt_list = calf1(test_sample[,\"sign\"],emt_result)\n\nme_base = get_advbase_set(my_sql,'20171230')\nme_base$sign = factor('0',levels = c('0','1'))\nme_set = me_base[,pickedColums]\nme_set = rbind(me_set,me_set)\nresult_em_user = em(modelName = mEst$modelName,data = me_set[,!(names(me_set)%in%c(\"sign\"))],\n                    parameters = mEst$parameters)\nem_result = ifelse(result_em_user$z[,1]-result_em_user$z[,2]>0,0,1)\nem_list = calf1(me_set[,\"sign\"],em_result)\nem_AUC = calAUC(me_set[,\"sign\"],em_result)\n\nBIC_TRAIN = mclustBIC(train_base[complete.cases(train_base),!(names(train_base)%in%c(\"sign\"))])\nBIC_TEST = mclustBIC(test_set[complete.cases(test_set),!(names(test_set)%in%c(\"sign\"))])\nBIC_M <- mclustBIC(train_set[complete.cases(train_set),!(names(train_set)%in%c(\"sign\"))])\n\nresult_nb = predict(model_nb,me_set[complete.cases(me_set),!(names(me_set) %in% \"sign\")])\nnb_list = calf1(me_set[,\"sign\"],result_nb)\nnb_f1 = nb_list$f1\n\nresult_random_forest = predict(model_random_forest,test_set[complete.cases(test_set),!(names(test_set) %in% \"sign\")])\nrf_list = calf1(test_set[complete.cases(test_set),]$sign,result_random_forest)\nrf_AUC = calAUC(test_set[complete.cases(test_set),]$sign,result_random_forest)\n\nresult_svm = predict(model_svm,x)\nsvm_list = calf1(test_set[complete.cases(test_set),]$sign,result_svm)\nsvm_AUC = calAUC(test_set[complete.cases(test_set),]$sign,result_svm)\n\ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n# train the model\nnbmodel <- train(sign~., data=train_base, method=\"naive_bayes\", preProcess=\"scale\", trControl=control,\n               importance = T)\n# estimate variable importance\nimportance <- varImp(nbmodel, scale=FALSE)\n# summarize importance\nprint(importance)\n\n\n\nmy_sql = \"select u_mid from dm.dm_app_pageview_info where u_mid like '9F96D1CD-108B-460B-B6C4-3E577AFE9271%'\"\n\nqz_sql = \"select u_mid from dm.dm_app_pageview_info where dt = '20171203' and l_city like '泉州%'\"\nqz_base = get_advbase_set(qz_sql)\nqz_base$sign = factor('1',levels = c('0','1'))\nqz_set = qz_base[,pickedColums]\nresult_nb = predict(model_nb,qz_set[complete.cases(qz_set),!(names(qz_set) %in% \"sign\")])\nView(cbind(qz_set[complete.cases(qz_set),!(names(qz_set) %in% \"sign\")],result_nb))\nnb_list = calf1(qz_set[,\"sign\"],result_nb)\nnb_f1 = nb_list$f1\n\n#沈阳11月5日，6日；11月9日，10日，12日，13日,29,30,12-02\n#沈阳1105 is normal(20%+ abnormal),1106 is normal(40%+ abnormal);the rest is abnormal\nsy_result = get_test_sample_result(\"沈阳\",20171202)\n#清远11月6,9,10,12,13\n#清远 all is abnormal\nqy_result = get_test_sample_result(\"清远\",20171112)\n#苏州 11月26,27,29,30,12-02,03\n#苏州 1126,1127,1129,1130 is normal(but the stay time is so consistent),\nsz_result = get_test_sample_result(\"苏州\",20171203)\n#上海 12-01,02,03,seems all is normal\nsh_result = get_test_sample_result(\"上海\",20171203)\n#12-02,03,09,10,11,12\n#漯河 all is abnormal(at least 80% is positive)\nlh_result = get_test_sample_result(\"漯河\",20171212)\n#温州1202 1203\n#温州1202 is abnormal,1203 is abnormal\nwz_result = get_test_sample_result(\"温州\",20171203)\n#南京 is normal\nnj_result = get_test_sample_result(\"南京\",20171226)\n#重庆 is normal\ncq_result = get_test_sample_result(\"重庆\",20171209)\n\nqy_1106_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '清远%' and dt = '20171106'\")\nqy_1109_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '清远%' and dt = '20171109'\")\nqy_1110_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '清远%' and dt = '20171110'\")\nqy_1112_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '清远%' and dt = '20171112'\")\nqy_1113_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '清远%' and dt = '20171113'\")\nqy_inter_umid = intersect(qy_1109_umid[[1]],qy_1110_umid[[1]])\nqy_inter_umid = Reduce(intersect,list(qy_1106_umid[[1]],qy_1109_umid[[1]],qy_1110_umid[[1]],qy_1112_umid[[1]],qy_1113_umid[[1]]))\n\nsh_1201_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '上海%' and dt = '20171201'\")\nsh_1202_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '上海%' and dt = '20171202'\")\nsh_1203_umid = read_data_impala_general(\"select distinct u_mid from dm.dm_app_pageview_info where \n                                        l_city like '上海%' and dt = '20171203'\")\n\nsh_inter_umid = Reduce(intersect,list(sh_1201_umid[[1]],sh_1202_umid[[1]],sh_1203_umid[[1]]))\n\n#so we have some thing unusual,no intersection for 清远\ntest_distribution_sql = \"select u_mid,avg(cast(stay_time AS bigint)) stay_time from\n(select u_mid,dt,l_city,p_stay_time stay_time from dm.dm_app_pageview_info where path = 'z' and platf_lv1 = 'APP') t1 inner join\n(select l_city,dt,count(*) num from dm.dm_app_pageview_info group by l_city,dt having num <20) t2\non t1.l_city = t2.l_city and t1.dt = t2.dt group by u_mid\"\n\ntest_distribution = read_data_impala_general(test_distribution_sql)\ntest_distrib = test_distribution[test_distribution$stay_time<80000,]\nggplot(data = test_distrib,aes(x = stay_time)) + geom_histogram()\n\nn = nrow(test_distrib)\nexp_sep = c(0,500,1000,2000,4000,8000,16000,32000,80000)\nlambda = 1/mean(test_distrib$stay_time)\nexp_theo = diff(pexp(exp_sep,rate = lambda))*n\nexp_real = table(cut(test_distrib$stay_time,breaks = exp_sep))\ntheo_real_diff = exp_theo - exp_real\n\na <- chisq.test(exp_real, p=exp_theo, rescale.p=TRUE, simulate.p.value=TRUE)\n\n#Cramér–von Mises criterion\nsample_size = 100000\nexp_theo_sample = rexp(sample_size,lambda)\nres <- CramerVonMisesTwoSamples(test_distrib$stay_time,exp_theo_sample)\npvalue = 1/6*exp(-res)\n\n#Kolmogorov-Smirnov test\nks.test(exp_theo_sample,test_distrib$stay_time)\n\nchisq.test(x=qy_result[[2]]$pv,y=qy_result[[2]]$avg_stay)\n\nx2 = train_set[,!(names(train_set) %in% \"sign\")]\ntrain_set$sign = as.factor(train_set$sign)\ny2 = train_set$sign\nmodel_nb2 = naive_bayes(sign~.,train_set[complete.cases(train_set),])\n\nqz_umid_sql = \"select distinct u_mid from dm.dm_app_pageview_info where l_city in \n('泉州','泉州市','quanzhou') and dt = '20171112' and path = 'z'\"\nwz_umid_sql = \"select distinct u_mid from dm.dm_app_pageview_info where l_city in \n('温州','温州市','wenzhou') and dt = '20171203' and path = 'z'\"\n#umid_sql = paste0(\"select distinct u_mid from (\",qz_umid_sql,\" union \",wz_umid_sql,\") df\")\n#neg_umid_sql = paste0(\"select distinct u_mid from (\",order_umid_sql,\" union \",last_long_umid_sql,\") df\")\ndateset = c(\"20171112\",\"20171203\")\n#negdateset should be the same, due to the function setting we don't need to specify the same date\numid_sql = c(qz_umid_sql,wz_umid_sql)\nneg_umid_sql = c(order_umid_sql,last_long_umid_sql)\ngeneral_pn = get_advbase_set(umid_sql,neg_umid_sql,dateset,renew = TRUE,complexity = TRUE)\ntrain_pn = preprocess_train_data(general_pn)\nmodel_nb_pn = naive_bayes(sign~.,train_pn[complete.cases(train_pn),])\n\ntest_umid_sql = \"select u_mid from dm.dm_app_pageview_info where l_city like '清远%' and dt = '20171112'\"\nqy_test = get_advbase_set(test_umid_sql,specDate = \"20171112\")\nqy_test = qy_test[,pickedColums]\n\n# write.xlsx(train_pn,\"~/data/test_abnormal_uv.xlsx\")\nlibrary(readxl)\ntrain_pn = read_xlsx(\"~/data/test_abnormal_uv.xlsx\")\n\nsource(\"~/R_Projects/SVM/Rfile/test_svm.R\")\nptm = proc.time()\ntest_svm_para = svm_para_class(train_pn,qy_test,\"sign\",calAR,\"recall\")\nsvm_time = proc.time() - ptm\nwrite.xlsx(test_svm_para,\"./Rdata/test_svm_para.xlsx\")\n\nptm <- proc.time()\nmodel_svm = svm(sign~.,train_pn[complete.cases(train_pn),])\nproc.time() - ptm\n#沈阳11月5日，6日；11月9日，10日，12日，13日,29,30,12-02\n#沈阳1105 is normal(20%+ abnormal),1106 is normal(40%+ abnormal);the rest is abnormal\n#recall:0.844\nsy_result = get_test_sample_result(\"沈阳\",20171202,test_model = model_nb_pn)\n#nb:0.967923\nsy_result = get_test_sample_result(\"沈阳\",20171202,test_model = model_svm)\n#svm:0.355\n\n#清远11月6,9,10,12,13\n#清远 all is abnormal:recall:0.994\nqy_result = get_test_sample_result(\"清远\",20171112,test_model = model_nb_pn)\n#nb:0.00043\nqy_result = get_test_sample_result(\"清远\",20171112,test_model = model_svm)\n#svm:0\n\n#苏州 11月26,27,29,30,12-02,03\n#苏州 1126,1127,1129,1130 is normal(but the stay time is so consistent),recall:0.124\nsz_result = get_test_sample_result(\"苏州\",20171203,test_model = model_nb_pn)\n#nb:0.00112\nsz_result = get_test_sample_result(\"苏州\",20171203,test_model = model_svm)\n#svm:0.000887\n\n#上海 12-01,02,03,seems all is normal:recall:0.1\nsh_result = get_test_sample_result(\"上海\",20171203,test_model = model_nb_pn)\n#nb:0.000383\nsh_result = get_test_sample_result(\"上海\",20171203,test_model = model_svm)\n#svm:0\n\n#12-02,03,09,10,11,12\n#漯河 all is abnormal(at least 80% is positive):recall:0.894\nlh_result = get_test_sample_result(\"漯河\",20171212,test_model = model_nb_pn)\n#nb:0.996\nlh_result = get_test_sample_result(\"漯河\",20171212,test_model = model_svm)\n#svm:0.981\n\n#温州1202 1203\n#温州1202 is abnormal,1203 is abnormal:recall:0.759\nwz_result = get_test_sample_result(\"温州\",20171203,test_model = model_nb_pn)\n#nb:0.807\nwz_result = get_test_sample_result(\"温州\",20171203,test_model = model_svm)\n#svm:0.759\n\n#南京 is normal recall:0.0312\nnj_result = get_test_sample_result(\"南京\",20171226,test_model = model_nb_pn)\n#nb:0.000488\nnj_result = get_test_sample_result(\"南京\",20171226,test_model = model_svm)\n#svm:0\n\n#重庆 is normal recall:0.0186\ncq_result = get_test_sample_result(\"重庆\",20171209,test_model = model_nb_pn)\n#nb:0.00772\ncq_result = get_test_sample_result(\"重庆\",20171209,test_model = model_svm)\n#svm:0.00585\n\n#泉州 \nqz_result = get_test_sample_result(\"泉州\",20171112,test_model = model_nb_pn)\n#nb:0.993\nqz_result = get_test_sample_result(\"泉州\",20171112,test_model = model_svm)\n#svm:0.991\n\n#result from my created sample is general the same result as the original trains\n\numid_sql_list = c(qz_umid_sql,wz_umid_sql)\ndate_list = c(20171112,20171203)\ntemp = get_base_with_restrict_union(umid_sql_list,date_list)\n\nsource(\"~/R_Projects/feature_selection/Rfile/feature_selection_fun.R\")\nsource(\"~/R_projects/ensemble_method/Rfile/Go_around_model_fun.R\")\nmodel_results = go_around_model(dataset = train_pn,test_formula = \"sign~.\")\nbwplot(model_results)\n# write.xlsx(model_results,\"./Rdata/model_results.xlsx\")\n\ntest_formula = as.formula(\"sign~.\")\ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"Accuracy\"\npreProcess=c(\"center\", \"scale\")\nptm <- proc.time()\nfit.lda <- train(test_formula, data=train_pn, method=\"lda\", metric=metric, preProc=c(\"center\", \"scale\"), trControl=control)\nlda_time = proc.time() - ptm\nlda_result = predict(object = fit.lda,newdata = qy_test)\nlda_result = predict(object = fit.lda,newdata = qy_test,type = \"prob\")\naccr = ifelse(lda_result$`1`>lda_result$`0`,1,0)\nlda_accr = length(accr[accr==1])/length(accr)\n\nptm <- proc.time()\nfit.c50 <- train(test_formula, data=train_pn, method=\"c50\", metric=metric, preProc=c(\"center\", \"scale\"), trControl=control)\nc50_time = proc.time() - ptm\nc50_result = predict(object = fit.c50,newdata = qy_test,type = \"prob\")\nlda_result = predict(object = fit.lda,newdata = qy_test)\n\nptm <- proc.time()\nfit.rf <- train(test_formula, data=train_pn, method=\"rf\", metric=metric, trControl=control)\nrf_time = proc.time() - ptm\n\nqy_test$sign = factor(qy_test$sign,level = c(\"0\",\"1\"))\nmodel_results2 = go_around_model_with_test(dataset = train_pn,newset = qy_test,keycol = \"sign\",calf = calAR)\n\nmodel_results2[[3]]\nformals(calAR)\n\nmodel_results3 = go_around_model_with_test_simple_version(dataset = train_pn,newset = qy_test,keycol = \"sign\",\nmodeldf = data.frame(modelname = c(\"lda\",\"rpart\",\"glm\",\"nb\")),calf = calAR,caltype = \"accuracy\",\nmetric = \"Accuracy\",preProc = c(\"center\",\"scale\"))\n\nCV_Folds <- createMultiFolds(y, k = 10, times = 3)\n\ntr_model = train(form = test_formula,data = dataset,method = md,metric = metric,trControl = trControl,preProc = preProc)\n\n# svmGrid <- expand.grid(sigma= 2^c(-25, -20, -15,-10, -5, 0), C= 2^c(0:5))\ntrControl = trainControl(method = \"none\",classProbs = TRUE,summaryFunction = twoClassSummary)\nsvmGrid = data.frame(sigma = 3.90625e-03, C = c(0.1,1,10,1000,10000))\n#svm_model = train(test_formula, data=train_pn, method=\"svmRadial\", metric=\"Accuracy\", preProc=c(\"center\", \"scale\"), trControl=control, fit=FALSE)\nsvm_model = train(test_formula,train_pn,method = \"svmRadial\",trControl = control,tuneGrid = svmGrid)\nsvm_cal = cal_model_accuracy(model = svm_model,newset = qy_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\nsvm_pred = predict(svm_model,qy_test)\n\nnb_model = train(test_formula,train_pn,method = \"nb\",trControl = control)\nnb_cal = cal_model_accuracy(model = nb_model,newset = qy_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\nnb_pred = predict(nb_model,qy_test)\n\nmodel_nb_pn = naive_bayes(test_formula,train_pn)\nnb_pred_pn = predict(model_nb_pn,qy_test)\nnb_pn_cal = cal_model_accuracy(model = model_nb_pn,newset = qy_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n\nlibrary(doMC)\nregisterDoMC(cores=8)\nptm = proc.time()\nnaive_bayes_model = train(test_formula,train_pn,method = \"naive_bayes\",trControl = control)\nnaive_bayes_cal = cal_model_accuracy(model = naive_bayes_model,newset = qy_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\nnaive_bayes_pred = predict(nb_model,qy_test)\nptm = proc.time() - ptm\n\nnaive_bayes_model = train(test_formula,train_pn,method = \"naive_bayes\",trControl = control)\n#long time\nsvm_model = train(test_formula,train_pn,method = \"svmRadial\",trControl = control,tuneGrid = svmGrid)\n\n#long time\nc50_model <- train(test_formula, data=train_pn, method=\"C5.0\", metric=metric, preProc=c(\"center\", \"scale\"), trControl=control)\n\n#short time\nrpart_model <- train(test_formula, data=train_pn, method=\"rpart\", metric=metric, trControl=control)\n\n#short time\nglm_model <- train(test_formula, data=train_pn, method=\"glm\", metric=metric, trControl=control)\n\n\n#沈阳11月5日，6日；11月9日，10日，12日，13日,29,30,12-02\n#沈阳1105 is normal(20%+ abnormal),1106 is normal(40%+ abnormal);the rest is abnormal\n#recall:0.844\nsy_test = get_test_sample(\"沈阳\",20171202)\nnaive_bayes_cal[\"shenyang\"] = cal_model_accuracy(model = naive_bayes_model,newset = sy_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n#nb:0.967923\n\n#清远11月6,9,10,12,13\n#清远 all is abnormal:recall:0.994\nqy_test = get_test_sample(\"清远\",20171112)\nnaive_bayes_cal[\"qingyuan\"] = cal_model_accuracy(model = naive_bayes_model,newset = qy_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\nqy_result = get_test_sample_result(\"清远\",20171112,test_model = model_svm)\n#svm:0\n\n#苏州 11月26,27,29,30,12-02,03\n#苏州 1126,1127,1129,1130 is normal(but the stay time is so consistent),recall:0.124\nsz_test = get_test_sample(\"苏州\",20171203)\nnaive_bayes_cal[\"suzhou\"] = cal_model_accuracy(model = naive_bayes_model,newset = sz_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\nsz_result = get_test_sample_result(\"苏州\",20171203,test_model = model_svm)\n#svm:0.000887\n\n#上海 12-01,02,03,seems all is normal:recall:0.1\nsh_test = get_test_sample(\"上海\",20171203)\nnaive_bayes_cal[\"shanghai\"] = cal_model_accuracy(model = naive_bayes_model,newset = sh_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n#nb:0.000383\nsh_result = get_test_sample_result(\"上海\",20171203,test_model = model_svm)\n#svm:0\n\n#12-02,03,09,10,11,12\n#漯河 all is abnormal(at least 80% is positive):recall:0.894\nlh_test = get_test_sample(\"漯河\",20171212)\nnaive_bayes_cal[\"luohe\"] = cal_model_accuracy(model = naive_bayes_model,newset = lh_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n#nb:0.996\nlh_result = get_test_sample_result(\"漯河\",20171212,test_model = model_svm)\n#svm:0.981\n\n#温州1202 1203\n#温州1202 is abnormal,1203 is abnormal:recall:0.759\nwz_test = get_test_sample(\"温州\",20171203)\nnaive_bayes_cal[\"wenzhou\"] = cal_model_accuracy(model = naive_bayes_model,newset = wz_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n#nb:0.807\nwz_result = get_test_sample_result(\"温州\",20171203,test_model = model_svm)\n#svm:0.759\n\n#南京 is normal recall:0.0312\nnj_test = get_test_sample(\"南京\",20171226)\nnaive_bayes_cal[\"nanjing\"] = cal_model_accuracy(model = naive_bayes_model,newset = nj_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n#nb:0.000488\nnj_result = get_test_sample_result(\"南京\",20171226,test_model = model_svm)\n#svm:0\n\n#重庆 is normal recall:0.0186\ncq_test = get_test_sample(\"重庆\",20171209)\nnaive_bayes_cal[\"chongqing\"] = cal_model_accuracy(model = naive_bayes_model,newset = cq_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n#nb:0.00772\ncq_result = get_test_sample_result(\"重庆\",20171209,test_model = model_svm)\n#svm:0.00585\n\n#泉州 \nqz_test = get_test_sample(\"泉州\",20171112)\nnaive_bayes_cal[\"quanzhou\"] = cal_model_accuracy(model = naive_bayes_model,newset = qz_test,keycol = \"sign\",calf = calAR,caltype = \"accuracy\")\n#nb:0.993\nqz_result = get_test_sample_result(\"泉州\",20171112,test_model = model_svm)\n#svm:0.991\n\ntest_set_names = ls(pattern = \"^.._test$\")\n# model_result #svm\nmodel_result_1 = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"svm_model\",\"c50_model\",\"rpart_model\",\"glm_model\"))\nlibrary(readxl)\nname_para = read_xlsx(\"~/data/name_para.xlsx\")\ncollect_test_set(name_para,factordf = 1,renew = TRUE)\n\nget_var_sample\n\nnaive_bayes_model2 = train(form = test_formula,data = train_pn2,method = \"naive_bayes\",metric = metric,\n                           trControl = control,preProcess = preProcess)\nmodel_result_2 = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"naive_bayes_model2\"))\n\ntrain_pn3 = get_var_sample(oringin_set = train_pn,negdiv = 0.2)\nnaive_bayes_model3 = train(form = test_formula,data = train_pn3,method = \"naive_bayes\",metric = metric,\n                           trControl = control,preProcess = preProcess)\nmodel_result_3 = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"naive_bayes_model2\",\"naive_bayes_model3\"))\n\ntemp = predict(naive_bayes_model,newdata = sy_test,type = \"prob\")\n\n#check why original one can't work for qy_test\nnaive_bayes_model1 = naive_bayes(sign~.,train_pn[complete.cases(train_pn),])\ntemp = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"naive_bayes_model1\",\"naive_bayes_model2\",\"naive_bayes_model3\"))\n\n\n#get everyday's feature set\ndateset = c(\"20180129\")\n#negdateset should be the same, due to the function setting we don't need to specify the same date\numid_sql = \"select distinct u_mid from dm.dm_app_pageview_info where  dt = '20180129' and path = 'z'\"\ngeneral_today = get_advbase_set(umid_sql,specDate = dateset)\ntrain_today = preprocess_train_data(general_today)\ntemp = predict(naive_bayes_model,newdata = train_today)\ntemp2 = predict(naive_bayes_model,newdata = train_today,type = \"class\")\ntoday_result = cbind(train_today,pred = temp)\n\nname_para = read_xlsx(\"~/data/name_para.xlsx\")\ncollect_test_set(name_para,factordf = 1,renew = FALSE)\ntest_set_names = ls(pattern = \"_test(\\\\d){4}$\")\nmodel_result_1_new = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"naive_bayes_model1\",\"naive_bayes_model2\",\"naive_bayes_model3\"))\n\ntrain_pn_simple = train_pn[,pickedColums]\nnaive_bayes_model = train(form = test_formula,data = train_pn,method = \"naive_bayes\",metric = metric,\n                               trControl = control,preProcess = preProcess)\nnaive_bayes_model_simp = train(form = test_formula,data = train_pn_simple,method = \"naive_bayes\",metric = metric,\n                           trControl = control,preProcess = preProcess)\npickedColums = c(\"max_stay\",\"min_stay\",\"avg_stay\",\"pv\",\"umid_count_same_ip\",\"sign\",\"city_perc\",\"md_perc\",\"mobile_city_count\",\"mobile_gps_count\")\nnaive_bayes_model_no_rate = train(form = test_formula,data = train_pn[,pickedColums],method = \"naive_bayes\",metric = metric,\n                               trControl = control,preProcess = preProcess)\n\npickedColums = c(\"max_stay\",\"min_stay\",\"avg_stay\",\"pv\",\"umid_count_same_ip\",\"sign\",\"city_perc\",\"md_perc\",\"mobile_city_count\",\"rate\")\nnaive_bayes_model_no_gps_count = train(form = test_formula,data = train_pn[,pickedColums],method = \"naive_bayes\",metric = metric,\n                                  trControl = control,preProcess = preProcess)\n\nmodel_result_simple = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"naive_bayes_model_simp\",\"naive_bayes_model_no_rate\",\"naive_bayes_model_no_gps_count\",\"naive_bayes_model_real\"))\n\nnaive_bayes_model_real = train(form = test_formula,data = train_pn,method = \"naive_bayes\",metric = metric,\n                          trControl = control,preProcess = preProcess)\nnaive_bayes_model_real_ori = naive_bayes(test_formula,train_pn)\nmodel_result_simple = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"naive_bayes_model_simp\",\"naive_bayes_model_no_rate\",\"naive_bayes_model_no_gps_count\",\"naive_bayes_model_real\",\"naive_bayes_model_real_ori\"))\n\n# transform of the data\nP = ecdf(train_pn$max_stay)\nP(50000)\ntemp = qnorm(P(train_pn$max_stay))\n\nsz_umid_sql = \"select distinct u_mid from dm.dm_app_pageview_info where l_city in ('苏州','苏州市') and dt = '20171203' and path = 'z'\"\nsh_umid_sql = \"select distinct u_mid from dm.dm_app_pageview_info where l_city in ('上海','上海市') and dt = '20171201' and path = 'z'\"\numid_sql = c(sz_umid_sql,sh_umid_sql)\ndateset = c(\"20171203\",\"20171201\")\ntemp =  get_complex_train_set(dateset = dateset,umid_sql = umid_sql)\nnaive_bayes_model_szsh = naive_bayes(test_formula,temp)\nmodel_result_all = test_around_set(test_set_names = test_set_names,model_names = c(\"naive_bayes_model\",\"naive_bayes_model_simp\",\"naive_bayes_model_no_rate\",\"naive_bayes_model_no_gps_count\",\"naive_bayes_model_real\",\"naive_bayes_model_real_ori\",\"naive_bayes_model_szsh\"))\n\ntrain_pn$sign = as.factor(train_pn$sign)\ntest_imp_model <- train(test_formula, data=train_pn, method=\"naive_bayes\", trControl=control,\n                    importance = T)\n# estimate variable importance\nimportance <- varImp(test_imp_model, scale=FALSE)\n\ntoday_result = pred_day_result(naive_bayes_model_no_rate)\ntoday_result = today_result[complete.cases(today_result),]\ntoday_result$u_mid = as.character(today_result$u_mid)\ntoday_result$dt = as.character(today_result$dt)\ntoday_result$normal_prob = round(today_result$normal_prob,2)\ntoday_result$abnorm_prob = round(today_result$abnorm_prob,2)\ntoday_result[] = lapply(today_result,as.character)\n\ndbSendUpdate(con,\"create table test.test_umid1 (name string,sign string,abnorm_prob string,normal_prob string) partitioned by (dt string)\")\ndbWriteTable(con,\"test.test_umid\",today_result,overwrite = TRUE,append = TRUE)\n\n# getOption(\"pkgType\")\n",
    "created" : 1515383494208.000,
    "dirty" : false,
    "encoding" : "GB18030",
    "folds" : "",
    "hash" : "379797106",
    "id" : "5FCD6A26",
    "lastKnownWriteTime" : 1518057596,
    "last_content_update" : 1518057596138,
    "path" : "~/R_Projects/abnormal_activity_analysis/Rfile/test.R",
    "project_path" : "Rfile/test.R",
    "properties" : {
        "docOutlineVisible" : "0",
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}
save(result_dt,'~/data/dianping.RData')
save(result_dt,file = '~/data/dianping.RData')
?load
library('readxl')
library(readxl)
result_dt_copy = readxl('./dianpingdata.xlsx')
result_dt_copy = read_xlsx('./dianpingdata.xlsx')
path.expand(".")
path.expand("./Rdata")
path.expand("./R")
dir.exists("./Rdata")
result_dt_copy = read_xlsx('./dianpindata.xlsx')
library(rvest)
library(data.table)
library(magrittr)
get_business_region2 = function(url = 'https://www.dianping.com/shopall/2/0#BDBlock'){
web = read_html(url,encoding = 'UTF-8')
# business_region = web%>%html_nodes('ul#hotregion')%>%html_nodes('li')%>%html_text()Bravia
# region = web%>%html_nodes('ul#hotregion')%>%html_nodes('div .box .shopallCate')%>%extract2(2)%>%html_text()
# region = web%>%html_nodes('div .box .shopallCate')%>%extract2(2)%>%
business_region = web%>%html_nodes('div .box .shopallCate')%>%html_nodes('li>a.B')%>%html_text()
# dt = data.table(region = business_region,link = business_link)
return(business_region)
}
business_region2 = get_business_region2()
url = 'https://www.dianping.com/shopall/2/0#BDBlock'
web = read_html(url,encoding = 'UTF-8')
business_region2 = web%>%html_nodes('div .box .shopallCate')%>%html_nodes('li>a.B')%>%html_text()
business_region2 = web%>%html_nodes('div .box shopallCate')%>%html_nodes('li>a.B')%>%html_text()
business_region2 = web%>%html_nodes('div .box')%>%html_nodes('li>a.B')%>%html_text()
business_region2 = web%>%html_nodes('div .box')%>%extract2(2)%>%html_nodes('li>a.B')%>%html_text()
business_region2
?html_nodes
business_region2 = web%>%html_nodes('div .box.shopallCate')%>%extract2(2)%>%html_nodes('li>a.B')%>%html_text()
business_region2
city_list
get_business_region2 = function(url = 'https://www.dianping.com/shopall/2/0#BDBlock'){
web = read_html(url,encoding = 'UTF-8')
business_region2 = web%>%html_nodes('div .box.shopallCate')%>%extract2(2)%>%html_nodes('li>a.B')%>%html_text()
return(business_region2)
}
dianping_scrapper2 = function(city_list){
v = vector(mode = 'character',length = 0)
result_dt = data.table(city=v,region = v)
for(city in city_list){
city_url = paste0('https://www.dianping.com/',city)
web = read_html(url,encoding = 'UTF-8')
business_groups_link = web%>%html_nodes('ul#hotregion')%>%html_nodes('li>a.more')%>%html_attr('href')
business_region2 = get_business_region2(business_groups_link)
temp_dt = data.table(city = city,region = business_region2)
result_dt = rbind(result_dt,temp_dt)
}
return(result_dt)
}
result1 = dianping_scrapper2(city_list)
dianping_scrapper2 = function(city_list){
v = vector(mode = 'character',length = 0)
result_dt = data.table(city=v,region = v)
for(city in city_list){
city_url = paste0('https://www.dianping.com/',city)
web = read_html(url,encoding = 'UTF-8')
business_groups_link = web%>%html_nodes('ul#hotregion')%>%html_nodes('li>a.more')%>%html_attr('href')
business_groups_link = paste0('https://www.dianping.com/',business_groups_link)
temp_dt = data.table(city = city,region = business_region2)
result_dt = rbind(result_dt,temp_dt)
}
return(result_dt)
}
result1 = dianping_scrapper2(city_list)
View(result1)
View(result1)
city_list
dianping_scrapper2 = function(city_list){
v = vector(mode = 'character',length = 0)
result_dt = data.table(city=v,region = v)
for(city in city_list){
city_url = paste0('https://www.dianping.com/',city)
web = read_html(city_url,encoding = 'UTF-8')
business_groups_link = web%>%html_nodes('ul#hotregion')%>%html_nodes('li>a.more')%>%html_attr('href')
business_groups_link = paste0('https://www.dianping.com/',business_groups_link)
business_region2 = get_business_region2(business_groups_link)
temp_dt = data.table(city = city,region = business_region2)
result_dt = rbind(result_dt,temp_dt)
}
return(result_dt)
}
result1 = dianping_scrapper2(city_list)
dianping_scrapper2 = function(city_list){
v = vector(mode = 'character',length = 0)
result_dt = data.table(city=v,region = v)
for(city in city_list){
city_url = paste0('https://www.dianping.com/',city)
web = read_html(city_url,encoding = 'UTF-8')
business_groups_link = web%>%html_nodes('ul#hotregion')%>%html_nodes('li>a.more')%>%html_attr('href')
business_groups_link = paste0('https://www.dianping.com',business_groups_link)
business_region2 = get_business_region2(business_groups_link)
temp_dt = data.table(city = city,region = business_region2)
result_dt = rbind(result_dt,temp_dt)
}
return(result_dt)
}
result1 = dianping_scrapper2(city_list)
View(result1)
city_list
city_list = c('shanghai','nanjing','suzhou')
result2 = dianping_scrapper2(city_list)
View(result2)
city_list = c('beijing','hangzhou','wuxi')
result3 = dianping_scrapper2(city_list)
View(result3)
result_dt2 = rbind(result1,result2,result3)
save(result_dt2,file = '~/data/result_dt2.xlsx')
result_dt3 = result_dt2[,city=gsub('beijing','北京')]
?gsub
result_dt3 = result_dt2[,city=gsub('beijing','北京',city)]
result_dt3 = gsub('beijing','北京',result_dt2)
result_dt3
rm(result_dt3)
result_dt3 = result_dt2
result_dt3[,city:=gsub('beijing','北京',city)]
View(result_dt3)
result_dt3[,city:=gsub('tianjin','天津',city)]
result_dt3[,city:=gsub('suzhou','苏州',city)]
result_dt3[,city:=gsub('hangzhou','杭州',city)]
result_dt3[,city:=gsub('shanghai','上海',city)]
result_dt3[,city:=gsub('chongqing','重庆',city)]
result_dt3[,city:=gsub('wuxi','无锡',city)]
result_dt3[,city:=gsub('shenzhen','深圳',city)]
result_dt3[,city:=gsub('changsha','长沙',city)]
result_dt3[,city:=gsub('nanjing','南京',city)]
save(result_dt3,'~/data/result_dt3.RData')
file.create('~/data/result_dt3.RData')
save(result_dt3,'~/data/result_dt3.RData')
save(result_dt3,file = '~/data/result_dt3.RData')
ln(2.6)
log(2.6)
?log
log10(2.5)
library(rvest)
library(data.table)
library(magrittr)
book_list
View(result_dt)
df
View(result_dt)
book_list_df = read.table(text = book_list,sep = "/", header = TRUE)
book_list_df = read.table(text = book_list,sep = "/")
View(book_list_df)
book_list_df = data.table(book_list)
View(book_list_df)
View(book_list_df)
str_split(book_list_df)
str_split(book_list_df,pattern = "/")
str_split(book_list,pattern = "/")
?apply
apply(book_list_df,1,str_split,"/")
search()
View(installed.packages())
View(available.packages())
library(rvest)
url<-"https://www.liepin.com/zhaopin/?init=1"
page<-read_html(url)
position<-page%>%html_nodes('ul.sojob_list div.sojob-item-main div.job-info,h3 a')%>%html_text(trim =TRUE)
position  #查看职位
position<- position[-41]   #删除第41个
position
link<- page %>% html_nodes('ul.sojob_list  div.job-info,h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
position<-web %>% html_nodes("div.pages_content") %>% html_text()
link1<-c(1:length(link))  #初始化一个和link长度相等的link1
for(i in 1:length(link))
link1[i]<-paste0(baseurl,link[[i]][1])
baseurl = "https://www.liepin.com"
for(i in 1:length(link))
link1[i]<-paste0(baseurl,link[[i]][1])
link1  #查看link1
link2<-link1[-41] #删除最后一行
link2    #查看link2
link<-link2    #将link2重新赋值给link
salary <- page %>% html_nodes('span.text-warning') %>% html_text()
salary
link<- page %>% html_nodes('ul.sojob_list  div.job-info h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
link<- page %>% html_nodes('ul.sojob_list  div.job-info,h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
link<- page %>% html_nodes('div.job-info,h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
link<- page %>% html_nodes('h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
link<- page %>% html_nodes('div.job-info h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
link<- page %>% html_nodes('ul.sojob_list div.job-info h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
link<- page %>% html_nodes('ul.sojob_list div.job-info')%>%html_attrs()
link[1] #读取数据，规定编码
position<-page%>%html_nodes('h3 a')%>%html_text(trim =TRUE)
position  #查看职位
position<- position[-41]   #删除第41个
link<- page %>% html_nodes('h3 a')%>%html_attrs()
link[1] #读取数据，规定编码
link1<-c(1:length(link))  #初始化一个和link长度相等的link1
for(i in 1:length(link))
link1[i]<-paste0(baseurl,link[[i]][1])
link1  #查看link1
link2<-link1[-41] #删除最后一行
link2    #查看link2
link<-link2    #将link2重新赋值给link
salary <- page %>% html_nodes('span.text-warning') %>% html_text()
salary
experience <- page %>% html_nodes('p.condition span') %>% html_text()
experience
edu<-page %>% html_nodes('span.edu') %>% html_text()
edu
html_nodes(page,"span.edu")
html_text(html_nodes(page,"span.edu"))
place <- page %>% html_nodes('p .area') %>% html_text()
place
experience = page %>% html_nodes("span:nth-child(4)") %>% html_text()
experience
experience = page %>% html_nodes("p.condition span:nth-child(4)") %>% html_text()
experience
alldata = cbind(position,salary,place,edu.experience,link)
alldata = cbind(position,salary,place,edu,experience,link)
View(alldata)
web = read_html(url)
name = web%>%html_nodes(".productTitle a")%>%html_text()
name[1]
name
name = web%>%html_nodes("p.productTitle a")%>%html_attr('title')
web%>%html_nodes("p.productTitle a")
url = "https://list.tmall.com/search_product.htm?q=%BA%CB%B5%AF&type=p&vmarket=&spm=875.7931836%2FB.a2227oh.d100&from=mallfp..pc_1_searchbutton"
web = read_html(url)
name = web%>%html_nodes("p.productTitle a")%>%html_attr('title')
name
price = web%>%html_nodes("p.productPrice em")%>%html_attr('title')
nums = web%>%html_nodes("p.productStatus em")%>%html_text()
tianmao_result = cbind(name,price,nums)
View(tianmao_result)
?html_session
web = read_html(url)
cast_star = web%>%html_nodes("#titleCast .itemprop span")%>%html_text()
url = "http://www.imdb.com/title/tt2250912/?ref_=nv_sr_1"
web = read_html(url)
url = "http://www.imdb.com/title/tt2250912/"
web = read_html(url)
cast_star = web%>%html_nodes("#titleCast .itemprop span")%>%html_text()
cast_star
characters = web%>%html_nodes("#titleCast .character div")%>%html_text()
characters
characters = gsub(characters,"\s+","")
characters = gsub(characters,"\\s+","")
?gsub
characters = web%>%html_nodes("#titleCast .character div")%>%html_text()
characters = gsub("\\s+","",characters)
cast_lines = data.frame(actor = cast_star,character = characters)
View(cast_lines)
reviews =  web%>%html_nodes("table") %>%.[[3]] %>%html_table()
web%>%html_nodes("table")
demo(package = "rvest")
demo("tripadvisor", package = "rvest", ask = TRUE)
demo("united", package = "rvest", ask = TRUE)
demo("united", package = "rvest", ask = FALSE)
demo("zillow", package = "rvest", ask = FALSE)
?submit_form
?html_node
baidu = html_session("https://www.baidu.com/")
search_form_fill = baidu%>%html_node("form[name = f]")%>%html_form()%>%set_values("nobel prize")
?read_html
baidu_url = "https://www.baidu.com/"
search_form_fill = read_html(baidu_url)%>%html_node("form[name = f]")%>%html_form()%>%set_values("nobel prize")
search_form_fill = read_html(baidu_url)%>%html_node("form#form")%>%html_form()%>%set_values("nobel prize")
search_form_fill = read_html(baidu_url)%>%html_node("#form")%>%html_form()%>%set_values("nobel prize")
?html_form
baidu_page = read_html(baidu_url)
baidu_page%>%html_node("input#su")%>%html_attr("value")
baidu_page%>%html_node("#su")%>%html_attr("value")
baidu_page%>%html_node("a.mnav")%>%html_text()
baidu_page
?follow_link
sina_url = "https://www.sina.com.cn"
sina = html_session(sina_url)
sina_sport = sina%>%follow_link("体育")
Sys.getlocale()
sina_pl = sina_sport%>%follow_link("英超")
sina_sport = sina%>%follow_link("体育")%>%html_nodes("div.phdnews_hdline a.linkRed")%>%html_text()
sina%>%follow_link("体育")%>%html_nodes("div.phdnews_hdline a.linkRed")
sina_sport = sina%>%follow_link("体育")%>%html_nodes("li:nth-child(1) h2")%>%html_text()
str(baidu_page)
str(sina)
next_baidu = baidu%>%follow_link("新闻")
View(name)
name
View(alldata)
link<- page %>% html_nodes('h3 a')%>%html_attrs()
library(rvest)
link<- page %>% html_nodes('h3 a')%>%html_attrs()
url<-"https://www.liepin.com/zhaopin/?init=1"
page<-read_html(url)
baseurl = "https://www.liepin.com"
link<- page %>% html_nodes('h3 a')%>%html_attrs()
?jump_to
login_url = "https://secure.indeed.com/account/login?service=my&hl=zh_CN&co=CN&continue=https%3A%2F%2Fcn.indeed.com%2F"
session = html_session(login_url)
form = html_form(read_html(login_url))[[1]]
html_form(read_html(login_url))
form = html_form(session)[[1]]
login_url = "https://secure.indeed.com/account/login?service=my&hl=zh_CN&co=CN&continue=https%3A%2F%2Fcn.indeed.com%2F"
session = html_session(login_url)
form = html_form(session)[[1]]
query = "data science"
loc = "beijing"
web <- session%>%jump_to("http://cn.indeed.com")
form <- html_form(web)[[1]]
form <- set_values(form, q = query, l = loc)
submit_form(session,form)
session <- html_session("http://cn.indeed.com")
form <- html_form(web)[[1]]
form <- set_values(form, q = query, l = loc)
submit_form(session,form)
success_search = submit_form(session,form)
session <- html_session("http://cn.indeed.com")
form <- html_form(web)[[1]]
form <- set_values(form, q = query, l = loc)
success_search = submit_form(session,form)
success_search$url
job_name = success_search%>%html_nodes("div.row a")%>%html_text()
View(as.data.frame(job_name))
job_name = success_search%>%html_nodes("a[data-tn-element=jobTitle]")%>%html_text()
job_name
job_name2 = success_search%>%follow_link(2)html_nodes("a[data-tn-element=jobTitle]")%>%html_text()
job_name2 = success_search%>%follow_link("2")html_nodes("a[data-tn-element=jobTitle]")%>%html_text()
success_search%>%follow_link("2")
test = success_search%>%follow_link("2")
job_name = test%>%html_nodes("a[data-tn-element=jobTitle]")%>%html_text()
job_name = success_search%>%html_nodes("a[data-tn-element=jobTitle]")%>%html_text()
?read_html
library(httr)
?GET
?save
names(.GlobalEnv)
list()
ls(0)
ls()
class(GET)
str(GET)
str(ls)
?ls
url = "www.sina.com.cn"
sina_web = read_html(url)
url = "https://www.sina.com.cn"
sina_web = read_html(url)
sina_url = "https://www.sina.com.cn"
sina = html_session(sina_url)
sina_sport = sina%>%follow_link("体育")%>%html_nodes("li:nth-child(1) h2")%>%html_text()
sina_sport = sina%>%follow_link("sport")%>%html_nodes("li:nth-child(1) h2")%>%html_text()
sina_sport = sina%>%follow_link("1")%>%html_nodes("li:nth-child(1) h2")%>%html_text()
?content
help(package = "roracle")
?user_agent
?getURLContent
library(RCurl)
?getURLContent
?GET
library(httr)
sina_raw = GET(sina_url,user_agent("Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0"))
sina = read_html(sina_raw)
sina_sport = sina%>%follow_link("体育")%>%html_nodes("li:nth-child(1) h2")%>%html_text()
?html_session
sina = html_session(sina_url,user_agent("Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0"))
sina_sport = sina%>%follow_link("体育")%>%html_nodes("li:nth-child(1) h2")%>%html_text()
sina = html_session(sina_url)
sina = html_session(sina_url,user_agent("Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0"))
sina_sport = sina%>%html_nodes(".nav-w:nth-child(2) ul:nth-child(1) b")%>%html_text()
sina = html_session(sina_url)
sina_sport = sina%>%html_nodes(".nav-w:nth-child(2) ul:nth-child(1) b")%>%html_text()
?follow_link
test_page = sina%>%follow_link("体育")
test_page = sina%>%follow_link("体育",user_agent("Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0"))
test_page = sina%>%follow_link("体育",user_agent = "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0")
tyre<- read.csv(file.choose(),header = TRUE, sep = ",")
attach(tyre)
tyre<- read.csv(file.choose(),header = TRUE, sep = ",")
attach(tyre)
View(tyre)
boxplot(Mileage~Brands, main="Fig.-1: Boxplot of Mileage of Four Brands of Tyre", col= rainbow(4))
library(ggplot2)
ggplot(tyre, aes(Brands,Mileage))+geom_boxplot(aes(col=Brands))+labs(title="Boxplot of Mileage of Four Brands of Tyre")
?aes
boxplot.stats(Mileage[Brands=="CEAT"])
model1<- aov(Mileage~Brands)
summary(model1)
model1
TukeyHSD(model1, conf.level = 0.99)
?TukeyHSD
plot(TukeyHSD(model1, conf.level = 0.99),las=1, col = "red")
library(glots)
library(gplots)
plotmeans(Mileage~Brands, main="Fig.-3: Mean Plot with 95% Confidence Interval", ylab = "Mileage run ('000 miles)", xlab = "Brands of Tyre")
par(mfrow=c(2,2))
plot(model1)
uhat<-resid(model1)
shapiro.test(uhat)
bartlett.test(Mileage~Brands)
library(car)
levene.test(Mileage~Brands)
url = "http://www.360doc.com/content/15/0605/00/22099567_475741839.shtml"
web = read_html(url)
table = web%>%html_table("table")
html_table
?html_table
table = web%>%html_nodes("table")%>%html_table()
table = web%>%html_nodes("table",fill = TRUE)%>%html_table()
table = web%>%html_nodes("table")%>%html_table(fill = TRUE)
web%>%html_nodes("table")
table = web%>%html_nodes("table")%>%`[[`(2)%>%html_table()
table = web%>%html_nodes("table")%>%`[[`(3)%>%html_table()
table = web%>%html_nodes("table")%>%`[[`(3)%>%html_table(header = TRUE)
View(table)
table2 = web%>%html_nodes("table")%>%`[[`(4)%>%html_table(header = TRUE)
table2 = web%>%html_nodes("table")%>%`[[`(4)%>%html_table(header = TRUE,fill = TRUE)
View(table2)
table2[1,]
colnames(table2) = table2[1,]
View(table2)
table2 = table2[-1,]
View(table2)
url<-"https://www.liepin.com/zhaopin/?init=1"
page<-read_html(url)
experience2 = page %>% html_nodes("p.condition span:nth-child(4)") %>% html_text()
View(table2)
1+1
`+`(1,1)
table[1,]
`[`(table,1)
-(1,2)
`-`(2,1)
x
x = 1:10
x
`[`(x,3)
x[3]
`[[`(x,3)
l
l = list(1,2,3)
l
l[1]
l[[1]]
l[2]
l[[2]]
x
2
x[3]
library(twitteR)
library(httr)
library(devtools)
library(scheduler)
library(base64enc)
library(openssl)
library(taskscheduleR)
proxy_url <- "http://127.0.0.1:61387/"
Sys.setenv(http_proxy = proxy_url, https_proxy = proxy_url, ftp_proxy = proxy_url)
api_key <- "2PYxocfAcY0CMVgw6LBg6TMQZ"
api_secret <- "0uWl3XONWwpHTpZTJZW8mB5cvw8hF2IGKL6tIfllk9oO9wYP6s"
access_token <- "74952092-lfKHNCAUFOamMGJ0JCO2KJDRA9LDyWbyK2taS0F0Y"
access_token_secret <- "ZC6SOH5dx5MPppcSZmcFnkknhbIlUh98HRxFRHpv1GAQL"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
proxy_url <- "http://127.0.0.1:61387/"
Sys.setenv(http_proxy = proxy_url, https_proxy = proxy_url, ftp_proxy = proxy_url)
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
trump_tweets = userTimeline("realDonaldTrump",n = 3200)
tweetdt.df <- twListToDF(trump_tweets)
View(tweetdt.df)
url = "https://movie.douban.com/subject/27024903/comments?status=P"
library(rvest)
library(magrittr)
web = read_html(url)
comments = web %>% html_nodes("div#comments p.sg_selected")%>% html_text()
comments = web %>% html_nodes("div#comments p")%>% html_text()
comments
s <- html_session(url)
s2 = s %>% follow_link("后页")
comments2 = s2 %>% html_nodes("div#comments p")%>% html_text()
comments2
cbind(comments,comments2)
inits <- html_session(url)
?vecotr
?vector
comments.df = data.frame(comments = vector(mode = "character",length = 0))
View(comments.df)
s[1] = inits
l=list()
l[[1]] = inits
comments.df = data.frame(comments = vector(mode = "character",length = 0))
for(i in 1:50){
comments = l[[i]] %>% html_nodes("div#comments p")%>% html_text()
temp_data_frame = data.frame(comments = comments)
comments.df = rbind(comments.df,temp_data_frame)
l[[i+1]] =  l[[i]] %>% follow_link("后页")
}
View(temp_data_frame)
View(webs)
View(comments.df)
ll
ll[[1]] = 5
ll = list()
ll[[1]] = 5
View(comments.df)
data.frame(name = c("zhanglizhi","lizhizhang","lizhizhi"))
data.frame(name = vector(mode = "character",length = 0))
paste(1,"")
paste(1,"",sep = "")
paste(2,"",sep = "")
